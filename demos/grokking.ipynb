{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Any\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from devinterp.zoo.arithmetic import ModularArithmeticConfig, ModularArithmetic\n",
    "from devinterp.zoo.transformer import TransformerConfig, Transformer\n",
    "from devinterp.utils import get_default_device\n",
    "from devinterp.data import Reduction\n",
    "from devinterp.slt.sampler import estimate_rlct\n",
    "from devinterp.evals import CombineEvaluators, Evaluator, RepeatEvaluator\n",
    "from devinterp.optim.schedulers import LRScheduler\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "MODULUS = 113\n",
    "\n",
    "trainset, testset = ModularArithmeticConfig(\n",
    "    operator=\"/\",\n",
    "    modulus=MODULUS,\n",
    "    seed=0,\n",
    "    split=0.4\n",
    ").factory_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evals\n",
    "\n",
    "def cross_entropy_last_token(outputs, targets, reduction: Reduction = \"sum\"):\n",
    "    \"\"\"\n",
    "    Wrapper around cross entropy loss because we only care about the last number predicted.\n",
    "    \"\"\"\n",
    "    # Only look at predictions of last numbers\n",
    "    outputs = outputs[:, -1]\n",
    "\n",
    "    # Compute individual and summed losses for final number\n",
    "    logprobs = F.log_softmax(outputs.to(torch.float32), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=targets.unsqueeze(1), dim=-1)\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = -torch.mean(prediction_logprobs)\n",
    "    elif reduction == \"sum\":\n",
    "        loss = -torch.sum(prediction_logprobs)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reduction argument.\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1024, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n",
    "def eval_loss_and_acc(model: nn.Module, *_) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, loader in zip([\"train\", \"test\"], [trainloader, testloader]):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x)\n",
    "\n",
    "            total += cross_entropy_last_token(y_hat, y, reduction=\"sum\").item()\n",
    "            correct += (y_hat[:, -1, :].max(dim=1).indices == y).sum().item()  # argmax doesn't work for device=mps\n",
    "\n",
    "        results[f\"{name}/loss\"] = total / len(loader.dataset)\n",
    "        results[f\"{name}/accuracy\"] = correct / len(loader.dataset)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_rlct(model: nn.Module, *_):\n",
    "    optimizer_kwargs = dict(\n",
    "        lr=1e-7, noise_level=1., weight_decay=3e-7, elasticity=10., temperature=\"adaptive\", num_samples=len(trainset)\n",
    "    )\n",
    "    return {\n",
    "        \"rlct\": estimate_rlct(model, trainloader, cross_entropy_last_token, 'sgld', optimizer_kwargs, num_draws=20, num_chains=5, num_burnin_steps=0, num_steps_bw_draws=1, cores=1, pbar=False)\n",
    "    }\n",
    "\n",
    "\n",
    "evals = CombineEvaluators([\n",
    "    eval_loss_and_acc,\n",
    "    RepeatEvaluator(eval_rlct, 5),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/devinterp/devinterp/utils.py:46: UserWarning: Number of steps in int_logspace is not 25, got 24.\n",
      "  warnings.warn(\n",
      "INFO:devinterp.learner:Logging to wandb enabled (project: grokking, entity: devinterp)\n",
      "INFO:devinterp.learner:batch_size: 256\n",
      "checkpointer_config:\n",
      "  bucket_name: null\n",
      "  device: cpu\n",
      "  local_root: ../\n",
      "  project_dir: div-mod-113\n",
      "criterion: cross_entropy\n",
      "device: mps\n",
      "logger_config:\n",
      "  entity: devinterp\n",
      "  metrics: null\n",
      "  out_file: null\n",
      "  project: grokking\n",
      "  run_id: null\n",
      "  stdout: false\n",
      "  use_df: false\n",
      "num_steps: 25000\n",
      "num_training_samples: 5107\n",
      "optimizer_config:\n",
      "  betas: !!python/tuple\n",
      "  - 0.9\n",
      "  - 0.98\n",
      "  elasticity: null\n",
      "  lr: 0.001\n",
      "  momentum: null\n",
      "  noise_level: null\n",
      "  num_samples: null\n",
      "  optimizer_type: AdamW\n",
      "  temperature: null\n",
      "  weight_decay: 0.2\n",
      "scheduler_config: null\n",
      "\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjqhoogland\u001b[0m (\u001b[33mdevinterp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Jesse/Projects/devinterp/demos/wandb/run-20230910_090325-k2jc34da</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devinterp/grokking/runs/k2jc34da' target=\"_blank\">exalted-sky-15</a></strong> to <a href='https://wandb.ai/devinterp/grokking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devinterp/grokking' target=\"_blank\">https://wandb.ai/devinterp/grokking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devinterp/grokking/runs/k2jc34da' target=\"_blank\">https://wandb.ai/devinterp/grokking/runs/k2jc34da</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from devinterp.learner import LearnerConfig\n",
    "\n",
    "model_config = TransformerConfig(d_vocab=MODULUS + 1)\n",
    "model = model_config.factory().to(device)\n",
    "\n",
    "learner_config = LearnerConfig(\n",
    "    num_training_samples=len(trainset),\n",
    "    batch_size=256,\n",
    "    num_steps=25_000,\n",
    "    criterion=\"cross_entropy\",\n",
    "    device=device,\n",
    "    optimizer_config={\n",
    "        \"optimizer_type\": \"AdamW\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0.2,\n",
    "        \"betas\": (0.9, 0.98),\n",
    "    },\n",
    "    logger_config={\n",
    "        \"project\": \"grokking\",\n",
    "        \"entity\": \"devinterp\",\n",
    "        \"logging_steps\": {\n",
    "            \"log_space\": 25,\n",
    "            \"linear_space\": 100,\n",
    "        },\n",
    "   },\n",
    "   checkpointer_config={\n",
    "        \"checkpoint_steps\": {\n",
    "            \"log_space\": 25,\n",
    "            \"linear_space\": 100,\n",
    "        },\n",
    "        # \"bucket\": \"devinterp\",\n",
    "        \"project_dir\": \"div-mod-113\",\n",
    "        \"local_root\": \"../\"\n",
    "   },\n",
    ")\n",
    "\n",
    "learner = learner_config.factory(\n",
    "    model=model,\n",
    "    dataset=trainset,\n",
    "    evaluator=evals\n",
    ")\n",
    "learner.criterion = cross_entropy_last_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d3cf088f5749ce9daf03a5e2c55c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 Batch 0/25000 Loss: ?.??????:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learner.save_checkpoint(0)\n",
    "learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
