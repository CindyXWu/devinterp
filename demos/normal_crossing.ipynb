{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "sys.path.insert(1, \"/home/paperspace/devinterp\") # TODO fix path\n",
    "\n",
    "from devinterp.slt.sampler import Sampler, SamplerConfig, estimate_rlct\n",
    "from torch.utils.data import TensorDataset\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(DEVICE))\n",
    "        loss = criterion(output, target.to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(train_loader)\n",
    "def train_one_batch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(DEVICE))\n",
    "        loss = criterion(output, target.to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    return train_loss\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data.to(DEVICE))\n",
    "            loss = criterion(output, target.to(DEVICE))\n",
    "            test_loss += loss.item()\n",
    "    model.train()\n",
    "    return test_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolyModel(nn.Module):\n",
    "    def __init__(self, powers):\n",
    "        super(PolyModel, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.tensor([1., 0.3],dtype=torch.float32, requires_grad=True))\n",
    "        self.powers = powers\n",
    "    def forward(self, x):\n",
    "        multiplied = torch.prod(self.weights**self.powers)\n",
    "        x = x*multiplied\n",
    "        return x\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "powers = torch.tensor([1, 2])\n",
    "model = PolyModel(powers)\n",
    "sigma = 0.5\n",
    "lr = 0.0001\n",
    "num_steps = 2000\n",
    "num_train_samples = 5000\n",
    "num_test_samples = 1000\n",
    "batch_size = num_train_samples\n",
    "w_true = torch.zeros_like(powers)\n",
    "\n",
    "x = torch.normal(0, 2, size=(num_train_samples,))\n",
    "y = sigma * torch.normal(0, 1, size=(num_train_samples,))\n",
    "train_data = TensorDataset(x, y)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# x_test = torch.normal(0, 2, size=(num_test_samples,))\n",
    "# y_test =  sigma * torch.normal(0, 1, size=(num_test_samples,))\n",
    "# test_data = TensorDataset(x_test, y_test)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "rlct_estimates = []\n",
    "models = []\n",
    "test_loss = 0.\n",
    "n_epochs = 20\n",
    "\n",
    "def print_rlcts(SGNHT_config, SGLD_config):\n",
    "    rlct_estimates_sgnht = []\n",
    "    rlct_estimates_sgdl = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        # test_loss = evaluate(model, test_loader, criterion)\n",
    "        # print(f\"Epoch {epoch+1}, Train Loss: {loss}, Test Loss: {test_loss}\")\n",
    "        # print(model.state_dict()['weights'])\n",
    "        model_for_rlct= copy.deepcopy(model)\n",
    "        model_for_rlct2= copy.deepcopy(model)\n",
    "        sgnht_sampler = Sampler(model_for_rlct, train_data, SGNHT_config)\n",
    "        sgdl_sampler = Sampler(model_for_rlct2, train_data, SGLD_config)\n",
    "        rlct_estimate_sgnht = sgnht_sampler.sample(summary_fn=estimate_rlct)\n",
    "        rlct_estimate_sgdl = sgdl_sampler.sample(summary_fn=estimate_rlct)\n",
    "        rlct_estimates_sgnht += [rlct_estimate_sgnht]\n",
    "        rlct_estimates_sgdl += [rlct_estimate_sgdl]\n",
    "        print(rlct_estimate_sgnht, rlct_estimate_sgdl)\n",
    "        # raise Exception\n",
    "    plt.hist(rlct_estimates_sgnht,alpha = 0.5, label='sgnht')\n",
    "    plt.hist(rlct_estimates_sgdl,alpha = 0.5, label='sgdl')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20992718636989594 0.015220989473164082\n",
      "0.28192073106765747 0.009325043298304081\n",
      "0.22444836795330048 0.01077716052532196\n",
      "0.23088666796684265 0.04624031484127045\n",
      "0.22681023180484772 0.0857624039053917\n",
      "0.23961687088012695 0.006228358950465918\n",
      "0.24253860116004944 0.01677807979285717\n",
      "0.24888941645622253 0.03492080047726631\n",
      "0.21274393796920776 0.034745845943689346\n",
      "0.22688022255897522 0.11567951738834381\n",
      "0.23842717707157135 0.04027438908815384\n",
      "0.24453306198120117 0.016025777906179428\n",
      "0.22892718017101288 0.03980201110243797\n",
      "0.23944191634655 0.08833422511816025\n",
      "0.25021907687187195 0.027992624789476395\n"
     ]
    }
   ],
   "source": [
    "# Initialize sgnht sampler\n",
    "SGNHT_config = SamplerConfig(\n",
    "    optimizer_config=dict(\n",
    "        optimizer_type=\"SGNHT\",\n",
    "        lr=lr,\n",
    "        diffusion_factor=0.01,\n",
    "        bounding_box_size=1.,\n",
    "        num_samples=len(train_data),\n",
    "        batch_size = batch_size,\n",
    "    ),\n",
    "    num_chains=1,\n",
    "    num_draws_per_chain=10_000,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_bw_draws=1,\n",
    "    batch_size=batch_size,         \n",
    "    criterion = 'mse_loss' \n",
    ")\n",
    "SGLD_config = SamplerConfig(\n",
    "    optimizer_config=dict(\n",
    "        optimizer_type=\"SGLD\",\n",
    "        lr=5*lr,\n",
    "        noise_level=.9,\n",
    "        # weight_decay=0.,\n",
    "        elasticity=1,\n",
    "        temperature='adaptive',\n",
    "        num_samples=len(train_data),\n",
    "    ),\n",
    "    num_chains=1,\n",
    "    num_draws_per_chain=10_000,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_bw_draws=1,\n",
    "    verbose=False,\n",
    "    batch_size=batch_size,         \n",
    "    criterion = 'mse_loss'\n",
    ")\n",
    "print_rlcts(SGNHT_config, SGLD_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
