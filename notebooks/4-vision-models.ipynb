{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation: ResNet-18\n",
    "\n",
    "\n",
    "1. Let's start by training a simple ResNet-18 model and take lots of checkpoints.\n",
    "2. Then do feature visualization on the end results (for a random sample of neurons). \n",
    "3. Look at how the activation of the target neuron reacts to those feature visualizations over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Container, Tuple, List, Dict, TypedDict\n",
    "from dataclasses import asdict\n",
    "import math\n",
    "from typing import Callable\n",
    "import functools\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb \n",
    "from torch.optim.lr_scheduler import LambdaLR \n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from devinterp.config import Config, OptimizerConfig, SchedulerConfig\n",
    "from devinterp.checkpoints import CheckpointManager\n",
    "from devinterp.logging import Logger\n",
    "from devinterp.learner import Learner\n",
    "from devinterp.misc.io import gen_images, show_images\n",
    "from devinterp.viz.activations import FeatureVisualizer, ActivationProbe\n",
    "from devinterp.misc.io import show_images\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "# wandb.finish()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/Jesse/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/Jesse/Projects/devinterp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/Jesse/Projects/devinterp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transforms)\n",
    "test_set = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/devinterp/devinterp/utils.py:45: UserWarning: Number of steps in int_logspace is not 100, got 88.\n",
      "  warnings.warn(f\"Number of steps in int_logspace is not {num}, got {len(result)}.\")\n",
      "/Users/Jesse/Projects/devinterp/devinterp/utils.py:45: UserWarning: Number of steps in int_logspace is not 25, got 24.\n",
      "  warnings.warn(f\"Number of steps in int_logspace is not {num}, got {len(result)}.\")\n",
      "INFO:devinterp.config:Logging to wandb enabled (project: resnet18, entity: devinterp)\n",
      "INFO:devinterp.config:batch_size: 128\n",
      "checkpoint_steps: !!set\n",
      "  0: null\n",
      "  1: null\n",
      "  2: null\n",
      "  3: null\n",
      "  6: null\n",
      "  10: null\n",
      "  15: null\n",
      "  25: null\n",
      "  40: null\n",
      "  63: null\n",
      "  100: null\n",
      "  159: null\n",
      "  252: null\n",
      "  401: null\n",
      "  636: null\n",
      "  1008: null\n",
      "  1600: null\n",
      "  2537: null\n",
      "  2666: null\n",
      "  4023: null\n",
      "  5333: null\n",
      "  6381: null\n",
      "  8000: null\n",
      "  10119: null\n",
      "  10666: null\n",
      "  13333: null\n",
      "  16000: null\n",
      "  16047: null\n",
      "  18666: null\n",
      "  21333: null\n",
      "  24000: null\n",
      "  25448: null\n",
      "  26666: null\n",
      "  29333: null\n",
      "  32000: null\n",
      "  34666: null\n",
      "  37333: null\n",
      "  40000: null\n",
      "  40357: null\n",
      "  42666: null\n",
      "  45333: null\n",
      "  48000: null\n",
      "  50666: null\n",
      "  53333: null\n",
      "  56000: null\n",
      "  58666: null\n",
      "  61333: null\n",
      "  64000: null\n",
      "device: cpu\n",
      "entity: devinterp\n",
      "logging_steps: !!set\n",
      "  0: null\n",
      "  1: null\n",
      "  2: null\n",
      "  3: null\n",
      "  4: null\n",
      "  5: null\n",
      "  6: null\n",
      "  7: null\n",
      "  8: null\n",
      "  9: null\n",
      "  10: null\n",
      "  11: null\n",
      "  13: null\n",
      "  14: null\n",
      "  16: null\n",
      "  18: null\n",
      "  20: null\n",
      "  22: null\n",
      "  25: null\n",
      "  28: null\n",
      "  31: null\n",
      "  35: null\n",
      "  40: null\n",
      "  44: null\n",
      "  50: null\n",
      "  55: null\n",
      "  62: null\n",
      "  69: null\n",
      "  78: null\n",
      "  87: null\n",
      "  97: null\n",
      "  109: null\n",
      "  122: null\n",
      "  136: null\n",
      "  152: null\n",
      "  171: null\n",
      "  191: null\n",
      "  213: null\n",
      "  239: null\n",
      "  267: null\n",
      "  299: null\n",
      "  334: null\n",
      "  374: null\n",
      "  418: null\n",
      "  467: null\n",
      "  523: null\n",
      "  585: null\n",
      "  646: null\n",
      "  654: null\n",
      "  731: null\n",
      "  818: null\n",
      "  914: null\n",
      "  1023: null\n",
      "  1144: null\n",
      "  1279: null\n",
      "  1292: null\n",
      "  1430: null\n",
      "  1600: null\n",
      "  1789: null\n",
      "  1939: null\n",
      "  2000: null\n",
      "  2237: null\n",
      "  2502: null\n",
      "  2585: null\n",
      "  2798: null\n",
      "  3128: null\n",
      "  3232: null\n",
      "  3499: null\n",
      "  3878: null\n",
      "  3912: null\n",
      "  4375: null\n",
      "  4525: null\n",
      "  4893: null\n",
      "  5171: null\n",
      "  5471: null\n",
      "  5818: null\n",
      "  6119: null\n",
      "  6464: null\n",
      "  6842: null\n",
      "  7111: null\n",
      "  7652: null\n",
      "  7757: null\n",
      "  8404: null\n",
      "  8557: null\n",
      "  9050: null\n",
      "  9569: null\n",
      "  9696: null\n",
      "  10343: null\n",
      "  10700: null\n",
      "  10989: null\n",
      "  11636: null\n",
      "  11966: null\n",
      "  12282: null\n",
      "  12929: null\n",
      "  13381: null\n",
      "  13575: null\n",
      "  14222: null\n",
      "  14868: null\n",
      "  14964: null\n",
      "  15515: null\n",
      "  16161: null\n",
      "  16734: null\n",
      "  16808: null\n",
      "  17454: null\n",
      "  18101: null\n",
      "  18713: null\n",
      "  18747: null\n",
      "  19393: null\n",
      "  20040: null\n",
      "  20686: null\n",
      "  20927: null\n",
      "  21333: null\n",
      "  21979: null\n",
      "  22626: null\n",
      "  23272: null\n",
      "  23402: null\n",
      "  23919: null\n",
      "  24565: null\n",
      "  25212: null\n",
      "  25858: null\n",
      "  26169: null\n",
      "  26505: null\n",
      "  27151: null\n",
      "  27797: null\n",
      "  28444: null\n",
      "  29090: null\n",
      "  29265: null\n",
      "  29737: null\n",
      "  30383: null\n",
      "  31030: null\n",
      "  31676: null\n",
      "  32323: null\n",
      "  32726: null\n",
      "  32969: null\n",
      "  33616: null\n",
      "  34262: null\n",
      "  34909: null\n",
      "  35555: null\n",
      "  36202: null\n",
      "  36596: null\n",
      "  36848: null\n",
      "  37494: null\n",
      "  38141: null\n",
      "  38787: null\n",
      "  39434: null\n",
      "  40080: null\n",
      "  40727: null\n",
      "  40925: null\n",
      "  41373: null\n",
      "  42020: null\n",
      "  42666: null\n",
      "  43313: null\n",
      "  43959: null\n",
      "  44606: null\n",
      "  45252: null\n",
      "  45765: null\n",
      "  45898: null\n",
      "  46545: null\n",
      "  47191: null\n",
      "  47838: null\n",
      "  48484: null\n",
      "  49131: null\n",
      "  49777: null\n",
      "  50424: null\n",
      "  51070: null\n",
      "  51178: null\n",
      "  51717: null\n",
      "  52363: null\n",
      "  53010: null\n",
      "  53656: null\n",
      "  54303: null\n",
      "  54949: null\n",
      "  55595: null\n",
      "  56242: null\n",
      "  56888: null\n",
      "  57231: null\n",
      "  57535: null\n",
      "  58181: null\n",
      "  58828: null\n",
      "  59474: null\n",
      "  60121: null\n",
      "  60767: null\n",
      "  61414: null\n",
      "  62060: null\n",
      "  62707: null\n",
      "  63353: null\n",
      "  64000: null\n",
      "num_epochs: 165\n",
      "num_steps: 64000\n",
      "num_training_samples: 50000\n",
      "optimizer_config:\n",
      "  lr: 0.1\n",
      "  momentum: 0.9\n",
      "  optimizer_type: SGD\n",
      "  weight_decay: 0.0001\n",
      "project: resnet18\n",
      "scheduler_config:\n",
      "  gamma: 0.5\n",
      "  last_epoch: -1\n",
      "  milestones:\n",
      "  - 16000\n",
      "  - 32000\n",
      "  - 48000\n",
      "  scheduler_type: MultiStepLR\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 128\n",
      "device: cpu\n",
      "entity: devinterp\n",
      "num_epochs: 165\n",
      "num_steps: 64000\n",
      "num_training_samples: 50000\n",
      "optimizer_config:\n",
      "  lr: 0.1\n",
      "  momentum: 0.9\n",
      "  optimizer_type: SGD\n",
      "  weight_decay: 0.0001\n",
      "project: resnet18\n",
      "scheduler_config:\n",
      "  gamma: 0.5\n",
      "  last_epoch: -1\n",
      "  milestones:\n",
      "  - 16000\n",
      "  - 32000\n",
      "  - 48000\n",
      "  scheduler_type: MultiStepLR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from devinterp.config import Config\n",
    "import yaml\n",
    "\n",
    "config = Config(\n",
    "    num_training_samples=len(train_set), \n",
    "    num_steps=64_000, \n",
    "    project=\"resnet18\", \n",
    "    entity=\"devinterp\", \n",
    "    logging_steps=(100, 100), \n",
    "    checkpoint_steps=(25, 25),\n",
    "    optimizer_config=OptimizerConfig(\n",
    "        optimizer_type=\"SGD\",\n",
    "        lr=0.1,\n",
    "        momentum=0.9,\n",
    "    ),\n",
    "    scheduler_config=SchedulerConfig(\n",
    "        scheduler_type=\"MultiStepLR\",\n",
    "        milestones=[16_000, 32_000, 48_000], \n",
    "        gamma=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(yaml.dump(config.model_dump(exclude=(\"logging_steps\", \"checkpoint_steps\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4uq1n32b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch/Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch/Loss</td><td>7.21299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-cloud-36</strong> at: <a href='https://wandb.ai/devinterp/resnet18/runs/4uq1n32b' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/4uq1n32b</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230815_120111-4uq1n32b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4uq1n32b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407c15176c5641a6b38e66400c3b7423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671020416682343, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/Projects/devinterp/notebooks/wandb/run-20230815_120151-kcc2oc13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devinterp/resnet18/runs/kcc2oc13' target=\"_blank\">deep-mountain-37</a></strong> to <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">https://wandb.ai/devinterp/resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devinterp/resnet18/runs/kcc2oc13' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/kcc2oc13</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a58d412c034c69825e11c1b72dffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 Batch 0/64000 Loss: ?.??????:   0%|          | 0/64000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss_metric(model, data, target, output):\n",
    "    return F.cross_entropy(output, target, reduction=\"sum\")\n",
    "\n",
    "def accuracy_metric(model, data, target, output):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    return pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "def eval_learner(learner: Learner):\n",
    "    return dataloaders_reduce(learner.model, {\"Train\": learner.train_loaders, \"Test\": learner.test_loaders}, {\"Loss\": loss_metric, \"Accuracy\": accuracy_metric}, device=learner.config.device)\n",
    "\n",
    "learner = Learner(model, train_set, test_set, config, metrics=[eval_learner])\n",
    "learner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature visualization\n",
    "\n",
    "We have a trained `model` (and a bunch of checkpoints). First, let's do some classic feature visualization on the final network. We'll select a few random neurons from ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, TypedDict, Union, Generic, TypeVar, Literal, Set\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "IDType = TypeVar(\"IDType\")\n",
    "\n",
    "class StorageProvider(Generic[IDType]):\n",
    "    \"\"\"\n",
    "    Wrapper for either local or cloud (S3) storage (or both).\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket to store checkpoints (Optional)\n",
    "    :param local_root: If provided, then the base directory in which to save files locally. If omitted, files will not be saved locally. (Optional)\n",
    "    :param save_locally: If True, saves checkpoints locally without deleting them (Optional)\n",
    "    \"\"\"\n",
    "    file_ids: List[IDType]\n",
    "\n",
    "    def __init__(self, bucket_name: Optional[str] = None, local_root: Optional[str] = None, parent_dir: str = \"data\",  device=torch.device(\"cpu\")):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.is_local_enabled = local_root is not None\n",
    "        self.local_root = Path(local_root or \"tmp\")\n",
    "        self.parent_dir = parent_dir\n",
    "        self.device = device\n",
    "\n",
    "        self.file_ids = [] # Any non-int hashable type.\n",
    "        self.client = None\n",
    "\n",
    "        # Cloud \n",
    "\n",
    "        if bucket_name and (os.getenv(\"AWS_SECRET_ACCESS_KEY\") and os.getenv(\"AWS_ACCESS_KEY_ID\")):\n",
    "            self.client = boto3.client(\"s3\")\n",
    "            self.file_ids = self.get_file_ids()\n",
    "        else:\n",
    "            warnings.warn(\"AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID must be set to use S3 bucket.\")\n",
    "\n",
    "        # Local \n",
    "        \n",
    "        local_path = os.path.join(self.local_root, parent_dir)\n",
    "        if self.is_local_enabled and not os.path.exists(local_path):\n",
    "            os.makedirs(local_path)\n",
    "        \n",
    "        if not self.bucket_name and not self.local_root:\n",
    "            warnings.warn(\"Neither S3 bucket name provided nor local_root is defined. Files will not be persisted.\")\n",
    "\n",
    "    @property\n",
    "    def is_s3_enabled(self):\n",
    "        return self.client is not None\n",
    "    \n",
    "    def id_to_name(self, file_id: Union[IDType, Literal[\"*\"]]) -> str:\n",
    "        \"\"\"Should contain no `/` and should handle the wildcard.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def id_to_key(self, file_id: Union[IDType, Literal[\"*\"]]) -> str:\n",
    "        return f\"{self.parent_dir}/{self.id_to_name(file_id)}.pt\"\n",
    "    \n",
    "    def name_to_id(self, name: str) -> IDType:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_file_ids(self) -> List[IDType]:\n",
    "        \"\"\"\n",
    "        Returns a list of tuples (epoch, batch_idx) of all checkpoints in the bucket or local directory.\n",
    "        \"\"\"\n",
    "        file_ids: Set[IDType] = set()\n",
    "\n",
    "        if self.is_local_enabled:\n",
    "            files = glob.glob(f\"{self.local_root}/{self.id_to_key('*')}\")\n",
    "            file_ids |= {self.name_to_id(os.path.basename(f)) for f in files}\n",
    "\n",
    "        if self.is_s3_enabled:\n",
    "            response = self.client.list_objects_v2(Bucket=self.bucket_name)\n",
    "            if \"Contents\" in response:\n",
    "                file_ids |= {self.name_to_id(item[\"Key\"]) for item in response[\"Contents\"] if item[\"Key\"].startswith(self.parent_dir)}\n",
    "            \n",
    "        return sorted(list(file_ids))\n",
    "\n",
    "    def upload_file(self, file_path: str, key: str):\n",
    "        self.client.upload_file(file_path, self.bucket_name, key)\n",
    "\n",
    "    def save_file(self, file_id: str, file):\n",
    "        file_path = self.id_to_key(file_id)\n",
    "        rel_file_path = self.local_root / file_path\n",
    "        torch.save(file, rel_file_path)\n",
    "\n",
    "        if self.client:\n",
    "            self.upload_file(rel_file_path, file_path)\n",
    "\n",
    "        if not self.is_local_enabled:\n",
    "            os.remove(rel_file_path)\n",
    "\n",
    "    def load_file(self, file_id):\n",
    "        file_path = self.id_to_key(file_id)\n",
    "        rel_file_path = self.local_root / file_path\n",
    "\n",
    "        print(file_id, file_path, rel_file_path)\n",
    "\n",
    "        if (self.is_local_enabled and os.path.exists(rel_file_path)):\n",
    "            logger.info(f\"Loading {file_path} from local save...\")\n",
    "        elif self.client:\n",
    "            logger.info(f\"Downloading {file_path} from bucket `{self.bucket_name}`...\")\n",
    "            self.client.download_file(self.bucket_name, file_path, rel_file_path)\n",
    "        else:\n",
    "            raise OSError(f\"File with id `{file_id}` not found either locally or in bucket.\")\n",
    "\n",
    "        checkpoint = torch.load(rel_file_path, map_location=self.device)\n",
    "\n",
    "        if not self.is_local_enabled and self.bucket_name and self.client:\n",
    "            os.remove(rel_file_path)\n",
    "\n",
    "        return checkpoint\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_id in self.file_ids:\n",
    "            yield self.load_file(file_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            return self.load_file(self.file_ids[idx])\n",
    "        \n",
    "        elif idx not in self.file_ids:\n",
    "            warnings.warn(f\"File with id `{idx}` not found in {self.bucket_name}.\")\n",
    "            return self.load_file(idx)\n",
    "\n",
    "        raise TypeError(f\"Invalid argument `{idx}` of type `{type(idx)}`\")\n",
    "\n",
    "    def __contains__(self, file_id):\n",
    "        return file_id in self.file_ids\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"StorageProvider({self.bucket_name}, {self.local_root})\"\n",
    "\n",
    "EpochAndBatch = Tuple[int, int]\n",
    "\n",
    "class CheckpointManager(StorageProvider[EpochAndBatch]):\n",
    "    def __init__(self, project_dir: str, bucket_name: Optional[str] = None, local_root: Optional[str] = None,  device=torch.device(\"cpu\")):\n",
    "        super().__init__(bucket_name, local_root, f\"checkpoints/{project_dir}\", device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def id_to_name(file_id: Union[EpochAndBatch, Literal[\"*\"]]) -> str:\n",
    "        if file_id == \"*\":\n",
    "            return \"*\"\n",
    "        \n",
    "        epoch, batch = file_id\n",
    "        return f\"checkpoint_epoch_{epoch}_batch_{batch}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def name_to_id(name: str) -> EpochAndBatch:\n",
    "        parts = name.split(\"_\")\n",
    "        epoch = int(parts[-3])\n",
    "        batch_idx = int(parts[-1].split(\".\")[0])\n",
    "        return epoch, batch_idx\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"CheckpointManager({self.parent_dir}, {self.bucket_name})\"\n",
    "\n",
    "\n",
    "NeuronSeedBatch = Tuple[int, int, int]\n",
    "\n",
    "class VisualizationManager(StorageProvider[NeuronSeedBatch]):\n",
    "    def __init__(self, project_dir: str, bucket_name: Optional[str] = None, local_root: Optional[str] = None,  device=torch.device(\"cpu\")):\n",
    "        super().__init__(bucket_name, local_root, f\"visualizations/{project_dir}\", device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def id_to_name(file_id: NeuronSeedBatch):\n",
    "        if file_id == \"*\":\n",
    "            return \"*\"\n",
    "        \n",
    "        neuron, seed, batch = file_id\n",
    "        return f\"visualization_neuron_{neuron}_seed_{seed}_batch_{batch}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def name_to_id(name: str) -> NeuronSeedBatch:\n",
    "        parts = name.split(\"_\")\n",
    "        neuron = int(parts[-5])\n",
    "        seed = int(parts[-3])\n",
    "        batch_idx = int(parts[-1].split(\".\")[0])\n",
    "        return neuron, seed, batch_idx\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"VisualizationManager({self.parent_dir}, {self.bucket_name})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/devinterp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/Jesse/Projects/devinterp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "INFO:__main__:Loading checkpoints/ResNet18/CIFAR10/checkpoint_epoch_164_batch_64000.pt from local save...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 64000) checkpoints/ResNet18/CIFAR10/checkpoint_epoch_164_batch_64000.pt ../checkpoints/ResNet18/CIFAR10/checkpoint_epoch_164_batch_64000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = CheckpointManager('ResNet18/CIFAR10', 'devinterp', local_root=\"..\")\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.load_state_dict(checkpoints[-1][\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "def worker(worker_id, viz, device, start, end, queue):\n",
    "    results = []\n",
    "\n",
    "    for i, probe in enumerate(viz.activations):\n",
    "        images = viz.render(\n",
    "            probe,\n",
    "            thresholds = thresholds,\n",
    "            verbose = verbose,\n",
    "            seed=init_seed + i,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            show_images(*images, **kwargs)\n",
    "\n",
    "        results.append((images, probe.activation))\n",
    "\n",
    "    queue.put(results)\n",
    "\n",
    "class FeatureVisualizer:\n",
    "    def __init__(self, model: torch.nn.Module, locations: Optional[list[str]]=None):\n",
    "        self.model = model\n",
    "        self.locations = locations or self.gen_locations(model)  # Defaults to all neurons in the model\n",
    "        self.activations = [ActivationProbe(model, location) for location in self.locations]\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_locations(model: torch.nn.Module, layer_type: Optional[Union[type, Tuple[type, ...]]] = None) -> list[str]:\n",
    "        \"\"\"Generate neurons of a particular kind of layer from a PyTorch model.\"\"\"\n",
    "        channel_locations = []\n",
    "\n",
    "        def recursive_search(module, prefix):\n",
    "            for name, submodule in module.named_children():\n",
    "                path = prefix + '.' + name if prefix else name\n",
    "\n",
    "                if not layer_type or isinstance(submodule, layer_type):\n",
    "\n",
    "                    # TODO: Get rid of the \"weight\"\n",
    "                    if isinstance(submodule, torch.nn.Linear):\n",
    "                        for feature in range(submodule.out_features):\n",
    "                            location = f\"{path}.weight.{feature}\"\n",
    "                            channel_locations.append(location)\n",
    "\n",
    "                    elif isinstance(submodule, torch.nn.Conv2d):\n",
    "                        for channel in range(submodule.out_channels):\n",
    "                            location = f\"{path}.weight.{channel}\"\n",
    "                            channel_locations.append(location)\n",
    "                    else:\n",
    "                        warnings.warn(f\"Unknown layer type: {type(submodule)}. Skipping\")\n",
    "\n",
    "\n",
    "                recursive_search(submodule, path)\n",
    "\n",
    "        recursive_search(model, '')\n",
    "\n",
    "        return channel_locations\n",
    "\n",
    "    def render(self, probe: ActivationProbe, transform: Optional[Transform] = None, thresholds: list[int]=[512], verbose: bool = True, seed: int = 0, device: torch.device = DEVICE) -> list[torch.Tensor]:\n",
    "        \"\"\"Renders an image that maximizes the activation of the specified neuron.\n",
    "        \n",
    "        Args:\n",
    "            transform (transforms.Compose, optional): Image transform to apply during optimization.\n",
    "            thresholds (list[int], optional): List of iterations at which to save the optimized image.\n",
    "            verbose (bool, optional): Whether to print progress information.\n",
    "            seed (int, optional): Random seed for initialization of the input image.\n",
    "            device (str, optional): Device on which to perform the computation.\n",
    "            \n",
    "        Returns:\n",
    "            tuple[list[torch.Tensor], float]: A tuple containing the final images and the activation value.\n",
    "        \"\"\"\n",
    "\n",
    "        # Assuming 'model' is your pre-trained ResNet model and 'location' is the string specifying the neuron's location\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        with probe.watch():\n",
    "            # Create a random image (1x3x224x224) to start optimization, with same size as typical ResNet input\n",
    "            torch.manual_seed(seed)\n",
    "            input_image = torch.rand((1, 3, 32, 32), requires_grad=True, device=device)\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = torch.optim.Adam([input_image], lr=0.01, weight_decay=1e-3)\n",
    "\n",
    "            final_images = []\n",
    "\n",
    "            # Optimization loop\n",
    "            pbar = range(max(thresholds) + 1)\n",
    "\n",
    "            if verbose:\n",
    "                pbar = tqdm(pbar, desc=f\"Visualizing {probe.location} (activation: ???)\")\n",
    "\n",
    "            for iteration in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                self.model(input_image)  # Forward pass through the model to trigger the hook\n",
    "                activation = probe.activation\n",
    "                loss = -activation  # Maximizing activation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if transform:\n",
    "                    input_image.data = transform(input_image.data.detach().clone())\n",
    "\n",
    "                if verbose:\n",
    "                    pbar.set_description(f\"Visualizing {probe.location} (activation: {activation.item():.2f})\")\n",
    "\n",
    "                if iteration in thresholds:\n",
    "                    image = input_image.detach().clone()\n",
    "                    image = torch.reshape(image, (1, 3, 32, 32))            \n",
    "                    final_images.append(image)\n",
    "        \n",
    "        return final_images\n",
    "\n",
    "    # def render(self, probe: ActivationProbe, num_visualizations: int, transform: Optional[Transform] = None, thresholds: list[int]=[512], verbose: bool = True, seed: int = 0, device: torch.device = DEVICE, diversity_weight: float = 0.1) -> list[list[torch.Tensor]]:\n",
    "    #     \"\"\"Renders multiple images that maximize the activation of the specified neuron, with a penalty to increase diversity.\n",
    "        \n",
    "    #     Args:\n",
    "    #         num_visualizations (int): Number of visualizations to generate.\n",
    "    #         ... other args same as render ...\n",
    "    #         diversity_weight (float, optional): Weight of the diversity penalty.\n",
    "        \n",
    "    #     Returns:\n",
    "    #         list[list[torch.Tensor]]: A list containing lists of final images for each visualization.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     self.model.to(device)\n",
    "    #     self.model.eval()\n",
    "        \n",
    "    #     with probe.watch():\n",
    "\n",
    "    #         # Create random images (num_visualizations x 3 x 32 x 32) to start optimization\n",
    "    #         torch.manual_seed(seed)\n",
    "    #         input_images = torch.rand((num_visualizations, 3, 32, 32), requires_grad=True, device=device)\n",
    "\n",
    "    #         # Optimizer\n",
    "    #         optimizer = torch.optim.Adam([input_images], lr=0.01, weight_decay=1e-3)\n",
    "\n",
    "    #         all_final_images = [[] for _ in range(num_visualizations)]\n",
    "\n",
    "    #         pbar = range(max(thresholds) + 1)\n",
    "    #         if verbose:\n",
    "    #             pbar = tqdm(pbar, desc=f\"Visualizing {probe.location} (activation: ???)\")\n",
    "\n",
    "    #         for iteration in pbar:\n",
    "    #             optimizer.zero_grad()\n",
    "    #             total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    #             for i in range(num_visualizations):\n",
    "    #                 self.model(input_images[i].unsqueeze(0))  # Forward pass through the model to trigger the hook\n",
    "    #                 total_loss += -probe.activation  # Maximizing activation\n",
    "\n",
    "    #                 # Calculate diversity penalty for the current image\n",
    "    #                 for j in range(num_visualizations):\n",
    "    #                     if i != j:\n",
    "    #                         diversity_loss = F.cosine_similarity(input_images[i].view(1, -1), input_images[j].view(1, -1))\n",
    "    #                         total_loss += diversity_weight * diversity_loss\n",
    "\n",
    "    #             total_loss.backward()\n",
    "    #             optimizer.step()\n",
    "\n",
    "    #             if transform:\n",
    "    #                 input_images.data = transform(input_images.data.detach().clone())\n",
    "\n",
    "    #             if verbose:\n",
    "    #                 pbar.set_description(f\"Visualizing {probe.location} (activation: {-total_loss.item():.2f})\")\n",
    "\n",
    "    #             if iteration in thresholds:\n",
    "    #                 for i in range(num_visualizations):\n",
    "    #                     image = input_images[i].detach().clone()\n",
    "    #                     image = torch.reshape(image, (3, 32, 32))\n",
    "    #                     all_final_images[i].append(image)\n",
    "\n",
    "    #     return all_final_images\n",
    "\n",
    "    def _render_all(self, thresholds: list[int]=[512], verbose: bool = True, init_seed: int = 0, device: str = \"cuda\", num_workers: int = 1, **kwargs) -> list[tuple[list[torch.Tensor], float]]:\n",
    "        results = []\n",
    "\n",
    "        for i, probe in enumerate(self.activations):\n",
    "            images = self.render(\n",
    "                probe,\n",
    "                thresholds = thresholds,\n",
    "                verbose = verbose,\n",
    "                seed=init_seed + i,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            if verbose: \n",
    "                show_images(*images, **kwargs)\n",
    "\n",
    "            results.append((images, probe.activation))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def render_all(self, thresholds: list[int] = [512], verbose: bool = True, init_seed: int = 0, device: str = \"cuda\", num_workers: int = 1, **kwargs) -> list[tuple[list[torch.Tensor], float]]:\n",
    "        if num_workers == 1:\n",
    "            return self._render_all(thresholds, verbose, init_seed, device, **kwargs)\n",
    "        \n",
    "        mp.set_start_method('spawn', force=True)    \n",
    "        devices = [torch.device(f\"cpu:{i}\") for i in range(num_workers)]\n",
    "\n",
    "        queue = mp.Queue()\n",
    "        processes = []\n",
    "\n",
    "        # Split the work between the workers\n",
    "        split_size = len(self) // num_workers\n",
    "        for worker_id in range(num_workers):\n",
    "            start_idx = worker_id * split_size\n",
    "            end_idx = start_idx + split_size if worker_id != num_workers - 1 else len(self)\n",
    "            p = mp.Process(target=worker, args=(worker_id, viz, devices[worker_id], start_idx, end_idx, queue))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        results = []\n",
    "        for _ in range(num_workers):\n",
    "            results += queue.get()\n",
    "\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.activations)\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, str]) -> ActivationProbe:\n",
    "        if isinstance(idx, int):\n",
    "            return self.activations[idx]\n",
    "        elif isinstance(idx, str):\n",
    "            return next(filter(lambda probe: probe.location == idx, self.activations))\n",
    "        else:\n",
    "            raise TypeError(f\"Invalid type for index: {type(idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "        exitcode = _main(fd, parent_sentinel)exitcode = _main(fd, parent_sentinel)\n",
      "\n",
      "                            ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "                     ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "AttributeErrorAttributeError: Can't get attribute 'worker' on <module '__main__' (built-in)>: Can't get attribute 'worker' on <module '__main__' (built-in)>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'worker' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m viz \u001b[39m=\u001b[39m FeatureVisualizer(model, [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlayer1.0.conv1.weight.\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)])\n\u001b[0;32m----> 2\u001b[0m image \u001b[39m=\u001b[39m viz\u001b[39m.\u001b[39;49mrender_all(thresholds\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m192\u001b[39;49m, \u001b[39m255\u001b[39;49m, \u001b[39m511\u001b[39;49m], device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, num_workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[46], line 215\u001b[0m, in \u001b[0;36mFeatureVisualizer.render_all\u001b[0;34m(self, thresholds, verbose, init_seed, device, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_workers):\n\u001b[0;32m--> 215\u001b[0m     results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m processes:\n\u001b[1;32m    218\u001b[0m     p\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m block \u001b[39mand\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sem\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:215\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m maxlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m maxlength \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnegative maxlength\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes(maxlength)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m buf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:413\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 413\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    414\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:378\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    376\u001b[0m remaining \u001b[39m=\u001b[39m size\n\u001b[1;32m    377\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 378\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    379\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "viz = FeatureVisualizer(model, [f'layer1.0.conv1.weight.{i}' for i in range(10)])\n",
    "image = viz.render_all(thresholds=[0, 64, 128, 192, 255, 511], device=\"cpu\", num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximally active neurons\n",
    "\n",
    "Let's go through all neurons in the model and rank them by their activation. We will then plot the top 10 most active neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5800"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = FeatureVisualizer.gen_locations(viz.model, (nn.Linear, nn.Conv2d))\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layer1.0.conv1.weight.0', 'layer1.0.conv1.weight.1', 'layer1.0.conv1.weight.2', 'layer1.0.conv1.weight.3', 'layer1.0.conv1.weight.4', 'layer1.0.conv1.weight.5', 'layer1.0.conv1.weight.6', 'layer1.0.conv1.weight.7', 'layer1.0.conv1.weight.8', 'layer1.0.conv1.weight.9']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'render_multiple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m section \u001b[39m=\u001b[39m conv_neurons[i:i\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(section)\n\u001b[0;32m----> 6\u001b[0m _results \u001b[39m=\u001b[39m render_multiple(model, \u001b[39m*\u001b[39msection, thresholds\u001b[39m=\u001b[39m[\u001b[39m256\u001b[39m], device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m show_images(\u001b[39m*\u001b[39m[images[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m (images, _) \u001b[39min\u001b[39;00m _results], dpi\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m      8\u001b[0m neurons_results\u001b[39m.\u001b[39mextend(_results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'render_multiple' is not defined"
     ]
    }
   ],
   "source": [
    "neurons_results =  []\n",
    "\n",
    "for i in range(0, len(conv_neurons), 10):\n",
    "    section = conv_neurons[i:i+10]\n",
    "    print(section)\n",
    "    _results = render_multiple(model, *section, thresholds=[256], device=\"cuda:0\", verbose=False)\n",
    "    show_images(*[images[-1] for (images, _) in _results], dpi=50)\n",
    "    neurons_results.extend(_results)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Saving results at {i}\")\n",
    "        torch.save(neurons_results, \"../visualizations/restnet-cifar10.pt\")\n",
    "        \n",
    "        # Print the 100 most activated neurons\n",
    "        print([(name, activation) for (name, (_, activation)) in sorted(zip(conv_neurons, neurons_results), key=lambda x: x[1][1], reverse=True)[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developmental analysis of a sample neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neuron = \"layer1.1.conv1.weights.7\"\n",
    "viz, activation = render(model, sample_neuron, seed=0)[-1]\n",
    "print(activation)\n",
    "show_images(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(checkpoints, desc=\"Looping checkpoints (activation: ???)\")\n",
    "activations = []\n",
    "\n",
    "for state_dict in pbar:\n",
    "    model.load_state_dict(state_dict)\n",
    "    extractor = ActivationExtractor(model, sample_neuron)\n",
    "    handle = extractor.register_hook()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model(viz) \n",
    "        activations.append(extractor.activation)\n",
    "    \n",
    "    pbar.set_description(f\"Looping checkpoints (activation: {extractor.activation.item():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([b for (_, b) in checkpoints.checkpoints][-5:], activations[-5:])\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do feature visualization at the very start, at 90  steps (where it reaches a minimum) at 5k steps where it's close to 0, at 8600, at 9000, and at the last step. \n",
    "\n",
    "# First let's get the closest checkpoints to these steps\n",
    "\n",
    "ideal_checkpoint_steps = [90, 5000, 8600, 9000, 9999]\n",
    "\n",
    "def get_closest_checkpoint(checkpoints: list[tuple[int, int]], step: int) -> int:\n",
    "    return min([chkpt for chkpt in checkpoints], key=lambda x: abs(x[1] - step))\n",
    "\n",
    "checkpoint_steps = [get_closest_checkpoint(checkpoints.checkpoints, step) for step in ideal_checkpoint_steps]\n",
    "checkpoint_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (epoch, batch_idx) in tqdm(checkpoint_steps, desc=\"Going through checkpoints\"):\n",
    "    model.load_state_dict(checkpoints[(epoch, batch_idx)])\n",
    "    vizs, activation = render(model, sample_neuron, seed=0, thresholds=[0, 64, 128, 256, 512], verbose=True)\n",
    "    show_images(*vizs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a whole set of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_results = torch.load(\"../visualizations/restnet-cifar10.pt\", map_location=torch.device('cpu'))\n",
    "viz_results = sorted([(name, a, img) for (img, a), name in zip(viz_results, conv_neurons)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "eps = 1e-4\n",
    "large_eps = 100\n",
    "\n",
    "activations = [a for _, a, _ in viz_results]\n",
    "\n",
    "print(\"# Negative activations: \", len([a for a in activations if a < 0]))\n",
    "print(\"# Zero activations: \", len([a for a in activations if a == 0]))\n",
    "print(\"# Insignificant positive activations: \", len([a for a in activations if 0 < a <= eps]))\n",
    "print(\"# Moderate positive activations: \", len([a for a in activations if eps < a <= large_eps]))\n",
    "print(\"# Large positive activations: \", len([a for a in activations if large_eps < a]))\n",
    "\n",
    "activations = [a for a in activations if a > eps]\n",
    "\n",
    "log_bins = np.logspace(np.log10(min(activations)),\n",
    "                       np.log10(max(activations)), num=10)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(activations, bins=log_bins)\n",
    "plt.xscale('log') # Optional, if you want the x-axis to be logarithmic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 5 from each category by random\n",
    "np.random.seed(2)\n",
    "\n",
    "sample_neurons = [\n",
    "    *np.random.choice([n for n, a, _ in viz_results if a < -eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if -eps <= a <= eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if eps < a <= large_eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if large_eps < a], size=5, replace=False),\n",
    "]\n",
    "print(sample_neurons)\n",
    "images = [imgs[-1] for n, _, imgs in viz_results if n in sample_neurons]\n",
    "show_images(\n",
    "    *images,\n",
    "    nrow=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_multiple(model: nn.Module, checkpoints: CheckpointManager, *locations: str, opt_steps: int = 512, **kwargs):\n",
    "    model.load_state_dict(checkpoints[-1])\n",
    "    model.eval()\n",
    "\n",
    "    final_vizs: dict[str, torch.Tensor] = {}\n",
    "    vizs: dict[str, list[torch.Tensor]] = {}\n",
    "    activations: dict[str, list[float]] = {}\n",
    "   \n",
    "    # Create the visualizations for the last checkpoint\n",
    "    for location, _location_vizs in zip(locations, tqdm(render_multiple(model, *locations, thresholds=[opt_steps], **kwargs), desc=\"Creating initial visualizations\")):\n",
    "        final_vizs[location] = _location_vizs[0][0]\n",
    "        vizs[location] = []\n",
    "        activations[location] = []\n",
    " \n",
    "    for i, state_dict in enumerate(tqdm(checkpoints, desc=\"Visiting checkpoints\")):\n",
    "        batch_idx = checkpoints.checkpoints[i][1]\n",
    "\n",
    "        # Render the visualization for the next checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        for location in locations:\n",
    "            viz = final_vizs[location]\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "            extractor = ActivationExtractor(model, location)\n",
    "            handle = extractor.register_hook()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model(viz) \n",
    "                activations[location].append(extractor.activation.item())\n",
    "\n",
    "            handle.remove()\n",
    "\n",
    "        wandb.log({f\"Activations/{location}\": activations[location][-1] for location in locations}, step=batch_idx, commit=False)\n",
    "            \n",
    "        # Visualize this checkpoint\n",
    "        if i % 20 or i == len(checkpoints) - 1:\n",
    "            for location, _location_vizs in zip(locations, tqdm(render_multiple(model, *locations, thresholds=[opt_steps], **kwargs), desc=f\"Creating visualizations for batch {batch_idx}\")):\n",
    "                viz = _location_vizs[0][0]\n",
    "                vizs[location].append(viz)\n",
    "                image_np = gen_images(viz)\n",
    "                image = wandb.Image(image_np, caption=f\"Optimized {location} at batch {batch_idx}\")\n",
    "\n",
    "                wandb.log({f\"Visualizations/{location}\": image}, step=batch_idx)\n",
    "\n",
    "    return vizs, activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()\n",
    "# run_id = input(\"Run ID: \")\n",
    "wandb.init(project=config.project, entity=config.entity)\n",
    "results = evolve_multiple(model, checkpoints, *sample_neurons, device=\"cpu\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['layer3.0.conv2.weight.206', 'layer3.0.conv1.weight.149', 'layer3.0.conv1.weight.118', 'layer3.0.conv2.weight.63', 'layer2.0.conv1.weight.102', 'layer2.0.conv2.weight.110', 'layer2.0.conv1.weight.15', 'layer3.1.conv1.weight.20', 'layer2.0.conv2.weight.19', 'layer3.0.conv1.weight.205', 'layer1.0.conv2.weight.54', 'layer2.0.conv2.weight.12', 'layer2.0.downsample.0.weight.99', 'layer1.0.conv2.weight.0', 'layer1.0.conv1.weight.47', 'layer1.0.conv2.weight.41', 'layer1.0.conv2.weight.51', 'layer3.0.downsample.0.weight.125', 'layer2.0.conv1.weight.108', 'layer1.1.conv2.weight.32']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
