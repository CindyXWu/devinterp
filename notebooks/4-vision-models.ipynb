{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation: ResNet-18\n",
    "\n",
    "\n",
    "1. Let's start by training a simple ResNet-18 model and take lots of checkpoints.\n",
    "2. Then do feature visualization on the end results (for a random sample of neurons). \n",
    "3. Look at how the activation of the target neuron reacts to those feature visualizations over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Container, Tuple\n",
    "from dataclasses import asdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/Jesse/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/Jesse/Projects/devinterp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/Jesse/Projects/devinterp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:23<00:00, 7285702.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_set = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(batch_size=128, lr=0.01, weight_decay=0.0001, num_epochs=163, logging_steps={0, 1, 37890, 2, 3, 4, 23046, 5, 6, 60937, 7, 8203, 8, 46093, 9, 10, 11, 521, 31250, 13, 14, 16, 16406, 18, 54296, 20, 1562, 22, 25, 39453, 28, 31, 24609, 35, 62500, 9765, 1019, 39, 47656, 32812, 44, 3117, 17968, 49, 14897, 55859, 3125, 41015, 55, 1594, 26171, 62, 11328, 49218, 36418, 69, 34375, 583, 19531, 57421, 78, 4687, 42578, 27734, 87, 12890, 50781, 35937, 97, 21093, 58984, 6250, 56939, 44140, 109, 29296, 1140, 14453, 52343, 122, 37500, 22656, 60546, 7812, 45703, 136, 11914, 30859, 652, 16015, 53906, 1171, 39062, 152, 24218, 62109, 6814, 9375, 47265, 32421, 17578, 170, 55468, 2734, 40625, 25781, 2229, 63670, 10937, 48828, 190, 33984, 19140, 18628, 57031, 4296, 42187, 27343, 12500, 213, 50390, 729, 35546, 20703, 58593, 5859, 2787, 43750, 50920, 28906, 14062, 238, 51953, 37109, 1783, 22265, 1275, 60156, 7421, 23292, 45312, 30468, 4358, 15625, 4873, 53515, 266, 781, 38671, 16658, 40723, 23828, 61718, 8984, 46875, 32031, 17187, 55078, 2343, 40234, 298, 25390, 815, 63281, 10546, 48437, 9528, 33593, 3897, 32568, 18750, 56640, 3906, 41796, 26953, 8521, 5450, 12109, 333, 50000, 35156, 20312, 58203, 5468, 20830, 43359, 28515, 13671, 51562, 36718, 21875, 59765, 373, 7031, 44921, 30078, 15234, 53125, 390, 38281, 23437, 61328, 8593, 912, 1425, 46484, 31640, 16796, 3485, 10654, 54687, 1953, 417, 39843, 25000, 62890, 10156, 48046, 33203, 18359, 56250, 3515, 2493, 41406, 26046, 26562, 7620, 29125, 11718, 49609, 1993, 34765, 6094, 19921, 466, 13322, 57812, 5078, 42968, 28125, 13281, 45537, 51171, 36328, 21484, 59375, 6640, 44531, 29687, 14843, 52734}, project='resnet18', entity='devinterp', device=device(type='cpu'), betas=(0.9, 0.999))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    batch_size: int = 128\n",
    "    lr: float = 0.01  # Starting lr\n",
    "    weight_decay: float = 0.0001\n",
    "    num_epochs: Optional[int] = None\n",
    "    logging_steps: Optional[Container] = None\n",
    "    project: Optional[str] = None\n",
    "    entity: Optional[str] = None\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    betas: Tuple[float, float] = (0.9, 0.999)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.num_epochs is None:\n",
    "            # Default to 64k steps\n",
    "            self.num_epochs = 64000 * self.batch_size // len(train_set)\n",
    "\n",
    "        if self.logging_steps is None or isinstance(self.logging_steps, int):\n",
    "            logging_steps = self.logging_steps\n",
    "            # By default: 1x per epoch\n",
    "            self.logging_steps = set([i * len(train_set) // self.batch_size for i in range(self.num_epochs)]) \n",
    "\n",
    "            if isinstance(logging_steps, int):\n",
    "                # Logscale from with self.logging_steps steps between 0 and num_epochs * len(train_data) // self.batch_size\n",
    "                self.logging_steps |= {int(i) for i in np.logspace(0, np.log10(self.num_epochs * len(train_set) // self.batch_size), logging_steps)}\n",
    "                \n",
    "config = Config(project=\"resnet18\", entity=\"devinterp\", logging_steps=100)\n",
    "steps = sorted(list(config.logging_steps))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gr791vqj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbff7182d0b74383a90c7e4c2f5ec27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.327687…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch/Loss</td><td>▆▆▃▃█▆▅▃▃▄▅▃▃▃▁▃▄▂▃▃▃▂▃▃▃▂▂▃▅▂▃▄▂▃▂▃▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch/Loss</td><td>1.45581</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-field-2</strong> at: <a href='https://wandb.ai/devinterp/resnet18/runs/gr791vqj' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/gr791vqj</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230810_155803-gr791vqj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gr791vqj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c30c92eaec14eae9a394a332601d3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0167448923670842, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Jesse/Projects/devinterp/notebooks/wandb/run-20230810_174858-iieamhxz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devinterp/resnet18/runs/iieamhxz' target=\"_blank\">woven-leaf-3</a></strong> to <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">https://wandb.ai/devinterp/resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devinterp/resnet18/runs/iieamhxz' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/iieamhxz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def maybe_initialize_wandb(project_name: Optional[str] = None, entity: Optional[str] = None):\n",
    "    if project_name:\n",
    "        import wandb\n",
    "        wandb.init(project=project_name, entity=entity)\n",
    "        return wandb\n",
    "    return None\n",
    "\n",
    "wandb = maybe_initialize_wandb(config.project, config.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LambdaLR.__init__() got an unexpected keyword argument 'milestones'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m optim\n\u001b[1;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mweight_decay, betas\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbetas)\n\u001b[0;32m----> 5\u001b[0m scheduler \u001b[39m=\u001b[39m LambdaLR(optimizer, milestones\u001b[39m=\u001b[39;49m[\u001b[39m32_000\u001b[39;49m, \u001b[39m48_000\u001b[39;49m], gamma\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)  \u001b[39m# Same as in the paper \u001b[39;00m\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_set, batch_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: LambdaLR.__init__() got an unexpected keyword argument 'milestones'"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay, betas=config.betas)\n",
    "scheduler = LambdaLR(optimizer, milestones=[32_000, 48_000], gamma=0.1)  # Same as in the paper \n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, train_loader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, scheduler, logging_steps: set,device: torch.device, num_epochs=10, project=\"resnet18\", **kwargs):\n",
    "    model.train()\n",
    "\n",
    "    # Calculate total number of batches\n",
    "    num_batches_per_epoch = len(train_loader)\n",
    "    total_batches = num_batches_per_epoch * num_epochs\n",
    "\n",
    "    pbar = tqdm(total=total_batches, desc=f\"Epoch 0 Batch 0/{total_batches} Loss: ?.??????\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for _batch_idx, (data, target) in enumerate(train_loader, 1):  # Start batch_idx from 1\n",
    "            batch_idx = num_batches_per_epoch * epoch + _batch_idx\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update progress bar description\n",
    "            pbar.set_description(f\"Epoch {epoch} Batch {batch_idx}/{total_batches} Loss: {loss.item():.6f}\")\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if wandb:\n",
    "                wandb.log({\"Batch/Loss\": loss.item()}, step=batch_idx)\n",
    "\n",
    "            # Log to wandb & save checkpoints according to log_steps\n",
    "            if batch_idx in logging_steps:\n",
    "                torch.save(model.state_dict(), f\"../checkpoints/{project}/checkpoint_epoch_{epoch}_batch_{batch_idx}.pt\")\n",
    "        \n",
    "        pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3836473ad44a4136a351026dde6be9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 Batch 0/63733 Loss: ?.??????:   0%|          | 0/63733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_loader, optimizer, scheduler, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49masdict(config))\n",
      "Cell \u001b[0;32mIn[53], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, scheduler, logging_steps, device, num_epochs, project, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, target)\n\u001b[0;32m---> 17\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Projects/devinterp/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/devinterp/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, scheduler, **asdict(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature visualization\n",
    "\n",
    "We have a trained `model` (and a bunch of checkpoints). First, let's do some classic feature visualization on the final network. We'll select a few random neurons from ac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
