{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation: ResNet-18\n",
    "\n",
    "\n",
    "1. Let's start by training a simple ResNet-18 model and take lots of checkpoints.\n",
    "2. Then do feature visualization on the end results (for a random sample of neurons). \n",
    "3. Look at how the activation of the target neuron reacts to those feature visualizations over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Container, Tuple, List, Dict\n",
    "from dataclasses import asdict\n",
    "import math\n",
    "import numpy as np\n",
    "import wandb \n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import functools\n",
    "from torch.optim.lr_scheduler import LambdaLR \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "from devinterp.config import Config, OptimizerConfig, SchedulerConfig\n",
    "from devinterp.checkpoints import CheckpointManager\n",
    "from devinterp.logging import Logger\n",
    "from devinterp.data import CustomDataLoader\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/paperspace/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/paperspace/Projects/devinterp/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paperspace/Projects/devinterp/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transforms)\n",
    "test_set = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 128\n",
      "device: cuda\n",
      "entity: devinterp\n",
      "num_epochs: 165\n",
      "num_steps: 64000\n",
      "num_training_samples: 50000\n",
      "optimizer_config:\n",
      "  lr: 0.1\n",
      "  momentum: 0.9\n",
      "  optimizer_type: SGD\n",
      "  weight_decay: 0.0001\n",
      "project: resnet18\n",
      "scheduler_config:\n",
      "  gamma: 0.5\n",
      "  last_epoch: -1\n",
      "  milestones:\n",
      "  - 16000\n",
      "  - 32000\n",
      "  - 48000\n",
      "  scheduler_type: MultiStepLR\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/Projects/devinterp/devinterp/utils.py:45: UserWarning: Number of steps in int_logspace is not 100, got 88.\n",
      "  warnings.warn(f\"Number of steps in int_logspace is not {num}, got {len(result)}.\")\n",
      "/home/paperspace/Projects/devinterp/devinterp/utils.py:45: UserWarning: Number of steps in int_logspace is not 25, got 24.\n",
      "  warnings.warn(f\"Number of steps in int_logspace is not {num}, got {len(result)}.\")\n"
     ]
    }
   ],
   "source": [
    "from devinterp.config import Config\n",
    "import yaml\n",
    "\n",
    "config = Config(\n",
    "    num_training_samples=len(train_set), \n",
    "    num_steps=64_000, \n",
    "    project=\"resnet18\", \n",
    "    entity=\"devinterp\", \n",
    "    logging_steps=(100, 100), \n",
    "    checkpoint_steps=(25, 25),\n",
    "    optimizer_config=OptimizerConfig(\n",
    "        optimizer_type=\"SGD\",\n",
    "        lr=0.1,\n",
    "        momentum=0.9,\n",
    "    ),\n",
    "    scheduler_config=SchedulerConfig(\n",
    "        scheduler_type=\"MultiStepLR\",\n",
    "        milestones=[16_000, 32_000, 48_000], \n",
    "        gamma=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(yaml.dump(config.model_dump(exclude=(\"logging_steps\", \"checkpoint_steps\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnerStateDict(TypedDict):\n",
    "    model: Dict\n",
    "    optimizer: Dict\n",
    "    scheduler: Optional[Dict]\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model: torch.nn.Module, train_set: torch.utils.data.Dataset, test_set: torch.utils.data.DataLoader, config: Config, metrics: Optional[List[Callable[['Learner'], Dict]]]=None):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.optimizer = config.optimizer_config.factory(model.parameters())\n",
    "\n",
    "        def lr_lambda(step: int):\n",
    "            if step < 400:\n",
    "                return 0.1\n",
    "            elif step < 32_000:\n",
    "                return 1.\n",
    "            elif step < 48_000:\n",
    "                return 0.1\n",
    "            else:\n",
    "                return 0.01\n",
    "\n",
    "        self.scheduler = LambdaLR(self.optimizer, lr_lambda=lr_lambda) # config.scheduler_config.factory(self.optimizer)\n",
    "        self.train_loader = CustomDataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_set, batch_size=config.batch_size, shuffle=False)\n",
    "        self.metrics = metrics or []\n",
    "        self.logger = Logger(project=config.project, entity=config.entity, logging_steps=config.logging_steps, metrics=[], out_file=None, use_df=False)\n",
    "        self.checkpoints = CheckpointManager(f\"{model.__class__.__name__}18/{self.train_loader.dataset.__class__.__name__}\", 'devinterp')  # TODO: read 18 automatically\n",
    "        \n",
    "    def measure(self):\n",
    "        return functools.reduce(lambda x, y: x | y, [metric(self) for metric in self.metrics], {})\n",
    "\n",
    "    def resume(self, batch_idx: Optional[int] = None):\n",
    "        if batch_idx is None:\n",
    "            epoch, batch_idx = self.checkpoints[-1]\n",
    "        else:\n",
    "            epoch, batch = min(self.checkpoints, key=lambda x: abs(x[1] - batch_idx))\n",
    "\n",
    "            if batch != batch_idx:\n",
    "                warnings.warn(f\"Could not find checkpoint with batch_idx {batch_idx}. Resuming from closest batch ({batch}) instead.\")\n",
    "\n",
    "        self.load(epoch, batch_idx)\n",
    "\n",
    "    def train(self, resume=False, run_id: Optional[str] = None):\n",
    "        if resume:\n",
    "            self.resume(resume, run_id)\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.last_epoch = self.config.num_steps_per_epoch * epoch + batch_idx\n",
    "\n",
    "        self.model.to(self.config.device)\n",
    "        self.model.train()\n",
    "\n",
    "        if self.config.is_wandb_enabled:\n",
    "            if resume and not run_id:\n",
    "                warnings.warn(\"Resuming from checkpoint but no run_id provided. Will not log to existing wandb run.\")\n",
    "            \n",
    "            if not run_id:\n",
    "                wandb.init(project=config.project, entity=config.entity)\n",
    "            else:\n",
    "                wandb.init(project=config.project, entity=config.entity, run_id=run_id)\n",
    "\n",
    "        pbar = tqdm(total=self.config.num_steps, desc=f\"Epoch 0 Batch 0/{self.config.num_steps} Loss: ?.??????\")\n",
    "        \n",
    "        for epoch in range(0, self.config.num_epochs):\n",
    "            self.set_seed(epoch)\n",
    "\n",
    "            for _batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                batch_idx = self.config.num_steps_per_epoch * epoch + _batch_idx\n",
    "                data, target = data.to(self.config.device), target.to(self.config.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # Update progress bar description\n",
    "                pbar.set_description(f\"Epoch {epoch} Batch {batch_idx}/{self.config.num_steps} Loss: {loss.item():.6f}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                if self.config.is_wandb_enabled:\n",
    "                    # TODO: Figure out how to make this work with Logger\n",
    "                    wandb.log({\"Batch/Loss\": loss.item()}, step=batch_idx)\n",
    "\n",
    "                # Log to wandb & save checkpoints according to log_steps\n",
    "                if batch_idx in self.config.checkpoint_steps:\n",
    "                    self.save_checkpoint(epoch, batch_idx)\n",
    "\n",
    "                if batch_idx in self.config.logging_steps:\n",
    "                    self.logger.log(self.measure(), step=batch_idx)\n",
    "                    self.model.train()\n",
    "\n",
    "            pbar.close()\n",
    "\n",
    "        if self.config.is_wandb_enabled:\n",
    "            wandb.finish()\n",
    "\n",
    "    def state_dict(self) -> LearnerStateDict:\n",
    "        return {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, checkpoint: LearnerStateDict):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        if self.scheduler is not None and checkpoint[\"scheduler\"] is not None:\n",
    "            self.scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int, batch_idx: int):\n",
    "        checkpoint = self.state_dict()\n",
    "        self.checkpoints.save_checkpoint(checkpoint, epoch, batch_idx)\n",
    "\n",
    "    def load_checkpoint(self, epoch: int, batch_idx: int):\n",
    "        checkpoint = self.checkpoints.load_checkpoint(epoch, batch_idx)\n",
    "        self.load_state_dict(checkpoint)\n",
    "\n",
    "    def set_seed(self, seed: int):\n",
    "        np.random.seed(epoch)\n",
    "        torch.manual_seed(epoch)\n",
    "        random.seed(epoch)\n",
    "        self.train_loader.shuffle_data(seed=epoch)\n",
    "\n",
    "        if \"cuda\" in str(self.config.device):\n",
    "            torch.cuda.manual_seed_all(epoch) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjqhoogland\u001b[0m (\u001b[33mdevinterp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/Projects/devinterp/notebooks/wandb/run-20230814_172443-kjnely4r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devinterp/resnet18/runs/kjnely4r' target=\"_blank\">vibrant-donkey-22</a></strong> to <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">https://wandb.ai/devinterp/resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devinterp/resnet18/runs/kjnely4r' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/kjnely4r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c566a3c83a4641b439a6883bb33211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 Batch 0/64000 Loss: ?.??????:   0%|          | 0/64000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_model(model: torch.nn.Module, loader: torch.utils.data.DataLoader, config: Config):\n",
    "    loss = torch.zeros(1, device=config.device)\n",
    "    correct = torch.zeros(1, device=config.device)\n",
    "    total = torch.zeros(1, device=config.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            output = model(data)\n",
    "            loss += F.cross_entropy(output, target, reduction=\"sum\")\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(data)\n",
    "        \n",
    "    loss /= total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "\n",
    "def eval_learner(learner: Learner):\n",
    "    train_loss, train_accuracy = eval_model(learner.model, learner.train_loader, learner.config)\n",
    "    test_loss, test_accuracy = eval_model(learner.model, learner.test_loader, learner.config)\n",
    "\n",
    "    return {\n",
    "        \"Train/Loss\": train_loss,\n",
    "        \"Train/Accuracy\": train_accuracy,\n",
    "        \"Test/Loss\": test_loss,\n",
    "        \"Test/Accuracy\": test_accuracy,\n",
    "    }\n",
    "\n",
    "learner = Learner(model, train_set, test_set, config, metrics=[eval_learner])\n",
    "learner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature visualization\n",
    "\n",
    "We have a trained `model` (and a bunch of checkpoints). First, let's do some classic feature visualization on the final network. We'll select a few random neurons from ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = CheckpointManager('resnet18/cifar10', 'devinterp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.load_state_dict(checkpoints[-1])\n",
    "# model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v1', pretrained=True)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"\n",
    "    With this version, you can use the ActivationExtractor with the following location formats:\n",
    "\n",
    "    - 'layer1.0.conv1.weight.3': Only channel is specified; y and x default to center.\n",
    "    - 'layer1.0.conv1.weight.3.2': Channel and y are specified; x defaults to center.\n",
    "    - 'layer1.0.conv1.weight.3..2': Channel and x are specified; y defaults to center.\n",
    "    - 'layer1.0.conv1.weight.3.2.2': Channel, y, and x are all specified.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, location):\n",
    "        self.activation = None\n",
    "        self.model = model\n",
    "        self.location = location.split('.')\n",
    "        self.layer_path = []\n",
    "        self.channel = None\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "\n",
    "        # Split the location into layer path and neuron indices\n",
    "        state_dict_keys = list(model.state_dict().keys())\n",
    "        for part in self.location:\n",
    "            self.layer_path.append(part)\n",
    "            path = '.'.join(self.layer_path)\n",
    "            \n",
    "            if any(key.startswith(path) for key in state_dict_keys):\n",
    "                continue\n",
    "            else:\n",
    "                self.layer_path.pop()\n",
    "                self.channel, *yx = map(int, self.location[len(self.layer_path):])\n",
    "                if yx:\n",
    "                    self.y = yx[0]\n",
    "                    if len(yx) > 1:\n",
    "                        self.x = yx[1]\n",
    "                break\n",
    "\n",
    "        # Get the target layer\n",
    "        self.layer = model\n",
    "        for part in self.layer_path[:-1]:\n",
    "            self.layer = getattr(self.layer, part)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        y = self.y if self.y is not None else output.size(2) // 2\n",
    "        x = self.x if self.x is not None else output.size(3) // 2\n",
    "\n",
    "        self.activation = output[0, self.channel, y, x]\n",
    "\n",
    "    def register_hook(self):\n",
    "        handle = self.layer.register_forward_hook(self.hook_fn)\n",
    "        return handle\n",
    "\n",
    "\n",
    "def gen_image(image: torch.Tensor):\n",
    "    # Process the optimized input\n",
    "    image = image.detach().cpu().squeeze(0)\n",
    "\n",
    "    image -= image.min()\n",
    "    image /= image.max()\n",
    "\n",
    "    # Create grid\n",
    "    grid_image = vutils.make_grid([image], nrow=1)\n",
    "\n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_image_np = grid_image.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "\n",
    "    return grid_image_np\n",
    "\n",
    "def show_image(image: torch.Tensor):\n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_image_np = gen_image(image)\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(5, 5))  # You can change the size as you prefer\n",
    "    plt.imshow(grid_image_np)\n",
    "    plt.axis('off') # to remove the axis\n",
    "    plt.show()\n",
    "\n",
    "def show_images(*images: torch.Tensor, nrow=None, **kwargs):\n",
    "    # Normalize images to [0,1] and create grid\n",
    "    images = [img - img.min() for img in images]\n",
    "    images = [img / img.max() for img in images]\n",
    "    images = [img.squeeze(0) for img in images]\n",
    "    \n",
    "    # Create grid\n",
    "    grid_image = vutils.make_grid(images, nrow=nrow or len(images))\n",
    "\n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_image_np = grid_image.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(15, 30), **kwargs)  # You can change the size as you prefer\n",
    "    plt.imshow(grid_image_np)\n",
    "    plt.axis('off') # to remove the axis\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_jitter(input_image, jitter_amount=2):\n",
    "    \"\"\"Applies jitter by randomly shifting the image.\"\"\"\n",
    "    if not jitter_amount:\n",
    "        return input_image\n",
    "\n",
    "    x_shift, y_shift = torch.randint(jitter_amount, -jitter_amount, (2,))\n",
    "    return torch.roll(input_image, shifts=(x_shift, y_shift), dims=(2, 3))\n",
    "\n",
    "def render(model: nn.Module, location: str, thresholds: list[int]=[512], verbose: bool = True, seed: int = 0, device: str = torch.device) -> tuple[list[torch.Tensor], float]:\n",
    "    # Assuming 'model' is your pre-trained ResNet model and 'location' is the string specifying the neuron's location\n",
    "    model.to(device)\n",
    "    model = model.eval()\n",
    "    extractor = ActivationExtractor(model, location)\n",
    "    handle = extractor.register_hook()\n",
    "\n",
    "    # Create a random image (1x3x224x224) to start optimization, with same size as typical ResNet input\n",
    "    torch.manual_seed(seed)\n",
    "    input_image = torch.rand((1, 3, 32, 32), requires_grad=True, device=device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam([input_image], lr=0.01, weight_decay=1e-3)\n",
    "    jitter_amount = 0\n",
    "\n",
    "    final_images = []\n",
    "\n",
    "    # Optimization loop\n",
    "    pbar = range(max(thresholds) + 1)\n",
    "\n",
    "    if verbose:\n",
    "        pbar = tqdm(pbar, desc=f\"Visualizing {location} (activation: ???)\")\n",
    "\n",
    "    for iteration in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        model(input_image)  # Forward pass through the model to trigger the hook\n",
    "        activation = extractor.activation\n",
    "        loss = -activation  # Maximizing activation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        input_image.data = add_jitter(input_image.data.detach().clone(), jitter_amount=-jitter_amount)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description(f\"Visualizing {location} (activation: {activation.item():.2f})\")\n",
    "\n",
    "        if iteration in thresholds:\n",
    "            # if verbose:\n",
    "            #     show_image(input_image)\n",
    "\n",
    "            image = input_image.detach().clone()\n",
    "            image = torch.reshape(image, (1, 3, 32, 32))            \n",
    "            final_images.append(image)\n",
    "\n",
    "    handle.remove()  # Remove the hook after the loop\n",
    "\n",
    "    return final_images, extractor.activation.item()\n",
    "\n",
    "\n",
    "def render_multiple(model: nn.Module, *locations: str, thresholds: list[int]=[512], verbose: bool = True, init_seed: int = 0, device: str = \"cuda\", **kwargs) -> list[tuple[list[torch.Tensor], float]]:\n",
    "    results = []\n",
    "\n",
    "    for i, location in enumerate(locations):\n",
    "        images, activation = render(\n",
    "            model, \n",
    "            location = location,\n",
    "            thresholds = thresholds,\n",
    "            verbose = verbose,\n",
    "            seed=init_seed + i,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        if verbose: \n",
    "            show_images(*images, **kwargs)\n",
    "\n",
    "        results.append((images, activation))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = render_multiple(\n",
    "    model,    \n",
    "    'layer1.0.conv1.weight.0',\n",
    "    'layer1.0.conv2.weight.1',\n",
    "    'layer1.1.conv1.weight.7',\n",
    "    'layer1.1.conv2.weight.4',\n",
    "    'layer2.0.conv1.weight.3',\n",
    "    'layer2.0.conv2.weight.2',\n",
    "    thresholds=[0, 64, 128, 256, 512],\n",
    "    verbose=False,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "show_images(*[images[-1] for (images, _) in results], dpi=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximally active neurons\n",
    "\n",
    "Let's go through all neurons in the model and rank them by their activation. We will then plot the top 10 most active neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "def gen_conv_neurons(model: nn.Module):\n",
    "    \"\"\"Generate convolutional neurons from a PyTorch model.\"\"\"\n",
    "    channel_locations = []\n",
    "\n",
    "    def recursive_search(module, prefix):\n",
    "        for name, submodule in module.named_children():\n",
    "            path = prefix + '.' + name if prefix else name\n",
    "\n",
    "            # Check if the submodule is a convolutional layer\n",
    "            if isinstance(submodule, Conv2d):\n",
    "                # Generate locations for all channels in this convolutional layer\n",
    "                for channel in range(submodule.out_channels):\n",
    "                    location = f\"{path}.weight.{channel}\"\n",
    "                    channel_locations.append(location)\n",
    "\n",
    "            # Recursively search through children\n",
    "            recursive_search(submodule, path)\n",
    "\n",
    "    recursive_search(model, '')\n",
    "\n",
    "    return channel_locations\n",
    "\n",
    "conv_neurons = gen_conv_neurons(model)[64:]\n",
    "print(conv_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_results =  []\n",
    "\n",
    "for i in range(0, len(conv_neurons), 10):\n",
    "    section = conv_neurons[i:i+10]\n",
    "    print(section)\n",
    "    _results = render_multiple(model, *section, thresholds=[256], device=\"cuda:0\", verbose=False)\n",
    "    show_images(*[images[-1] for (images, _) in _results], dpi=50)\n",
    "    neurons_results.extend(_results)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Saving results at {i}\")\n",
    "        torch.save(neurons_results, \"../visualizations/restnet-cifar10.pt\")\n",
    "        \n",
    "        # Print the 100 most activated neurons\n",
    "        print([(name, activation) for (name, (_, activation)) in sorted(zip(conv_neurons, neurons_results), key=lambda x: x[1][1], reverse=True)[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developmental analysis of a sample neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neuron = \"layer1.1.conv1.weights.7\"\n",
    "viz, activation = render(model, sample_neuron, seed=0)[-1]\n",
    "print(activation)\n",
    "show_image(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(checkpoints, desc=\"Looping checkpoints (activation: ???)\")\n",
    "activations = []\n",
    "\n",
    "for state_dict in pbar:\n",
    "    model.load_state_dict(state_dict)\n",
    "    extractor = ActivationExtractor(model, sample_neuron)\n",
    "    handle = extractor.register_hook()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model(viz) \n",
    "        activations.append(extractor.activation)\n",
    "    \n",
    "    pbar.set_description(f\"Looping checkpoints (activation: {extractor.activation.item():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([b for (_, b) in checkpoints.checkpoints][-5:], activations[-5:])\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do feature visualization at the very start, at 90  steps (where it reaches a minimum) at 5k steps where it's close to 0, at 8600, at 9000, and at the last step. \n",
    "\n",
    "# First let's get the closest checkpoints to these steps\n",
    "\n",
    "ideal_checkpoint_steps = [90, 5000, 8600, 9000, 9999]\n",
    "\n",
    "def get_closest_checkpoint(checkpoints: list[tuple[int, int]], step: int) -> int:\n",
    "    return min([chkpt for chkpt in checkpoints], key=lambda x: abs(x[1] - step))\n",
    "\n",
    "checkpoint_steps = [get_closest_checkpoint(checkpoints.checkpoints, step) for step in ideal_checkpoint_steps]\n",
    "checkpoint_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (epoch, batch_idx) in tqdm(checkpoint_steps, desc=\"Going through checkpoints\"):\n",
    "    model.load_state_dict(checkpoints[(epoch, batch_idx)])\n",
    "    vizs, activation = render(model, sample_neuron, seed=0, thresholds=[0, 64, 128, 256, 512], verbose=True)\n",
    "    show_images(*vizs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a whole set of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_results = torch.load(\"../visualizations/restnet-cifar10.pt\", map_location=torch.device('cpu'))\n",
    "viz_results = sorted([(name, a, img) for (img, a), name in zip(viz_results, conv_neurons)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "eps = 1e-4\n",
    "large_eps = 100\n",
    "\n",
    "activations = [a for _, a, _ in viz_results]\n",
    "\n",
    "print(\"# Negative activations: \", len([a for a in activations if a < 0]))\n",
    "print(\"# Zero activations: \", len([a for a in activations if a == 0]))\n",
    "print(\"# Insignificant positive activations: \", len([a for a in activations if 0 < a <= eps]))\n",
    "print(\"# Moderate positive activations: \", len([a for a in activations if eps < a <= large_eps]))\n",
    "print(\"# Large positive activations: \", len([a for a in activations if large_eps < a]))\n",
    "\n",
    "activations = [a for a in activations if a > eps]\n",
    "\n",
    "log_bins = np.logspace(np.log10(min(activations)),\n",
    "                       np.log10(max(activations)), num=10)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(activations, bins=log_bins)\n",
    "plt.xscale('log') # Optional, if you want the x-axis to be logarithmic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 5 from each category by random\n",
    "np.random.seed(2)\n",
    "\n",
    "sample_neurons = [\n",
    "    *np.random.choice([n for n, a, _ in viz_results if a < -eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if -eps <= a <= eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if eps < a <= large_eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if large_eps < a], size=5, replace=False),\n",
    "]\n",
    "print(sample_neurons)\n",
    "images = [imgs[-1] for n, _, imgs in viz_results if n in sample_neurons]\n",
    "show_images(\n",
    "    *images,\n",
    "    nrow=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_multiple(model: nn.Module, checkpoints: CheckpointManager, *locations: str, opt_steps: int = 512, **kwargs):\n",
    "    model.load_state_dict(checkpoints[-1])\n",
    "    model.eval()\n",
    "\n",
    "    final_vizs: dict[str, torch.Tensor] = {}\n",
    "    vizs: dict[str, list[torch.Tensor]] = {}\n",
    "    activations: dict[str, list[float]] = {}\n",
    "   \n",
    "    # Create the visualizations for the last checkpoint\n",
    "    for location, _location_vizs in zip(locations, tqdm(render_multiple(model, *locations, thresholds=[opt_steps], **kwargs), desc=\"Creating initial visualizations\")):\n",
    "        final_vizs[location] = _location_vizs[0][0]\n",
    "        vizs[location] = []\n",
    "        activations[location] = []\n",
    " \n",
    "    for i, state_dict in enumerate(tqdm(checkpoints, desc=\"Visiting checkpoints\")):\n",
    "        batch_idx = checkpoints.checkpoints[i][1]\n",
    "\n",
    "        # Render the visualization for the next checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        for location in locations:\n",
    "            viz = final_vizs[location]\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "            extractor = ActivationExtractor(model, location)\n",
    "            handle = extractor.register_hook()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model(viz) \n",
    "                activations[location].append(extractor.activation.item())\n",
    "\n",
    "            handle.remove()\n",
    "\n",
    "        wandb.log({f\"Activations/{location}\": activations[location][-1] for location in locations}, step=batch_idx, commit=False)\n",
    "            \n",
    "        # Visualize this checkpoint\n",
    "        if i % 20 or i == len(checkpoints) - 1:\n",
    "            for location, _location_vizs in zip(locations, tqdm(render_multiple(model, *locations, thresholds=[opt_steps], **kwargs), desc=f\"Creating visualizations for batch {batch_idx}\")):\n",
    "                viz = _location_vizs[0][0]\n",
    "                vizs[location].append(viz)\n",
    "                image_np = gen_image(viz)\n",
    "                image = wandb.Image(image_np, caption=f\"Optimized {location} at batch {batch_idx}\")\n",
    "\n",
    "                wandb.log({f\"Visualizations/{location}\": image}, step=batch_idx)\n",
    "\n",
    "    return vizs, activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()\n",
    "# run_id = input(\"Run ID: \")\n",
    "wandb.init(project=config.project, entity=config.entity)\n",
    "results = evolve_multiple(model, checkpoints, *sample_neurons, device=\"cpu\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['layer3.0.conv2.weight.206', 'layer3.0.conv1.weight.149', 'layer3.0.conv1.weight.118', 'layer3.0.conv2.weight.63', 'layer2.0.conv1.weight.102', 'layer2.0.conv2.weight.110', 'layer2.0.conv1.weight.15', 'layer3.1.conv1.weight.20', 'layer2.0.conv2.weight.19', 'layer3.0.conv1.weight.205', 'layer1.0.conv2.weight.54', 'layer2.0.conv2.weight.12', 'layer2.0.downsample.0.weight.99', 'layer1.0.conv2.weight.0', 'layer1.0.conv1.weight.47', 'layer1.0.conv2.weight.41', 'layer1.0.conv2.weight.51', 'layer3.0.downsample.0.weight.125', 'layer2.0.conv1.weight.108', 'layer1.1.conv2.weight.32']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
