{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation: ResNet-18\n",
    "\n",
    "\n",
    "1. Let's start by training a simple ResNet-18 model and take lots of checkpoints.\n",
    "2. Then do feature visualization on the end results (for a random sample of neurons). \n",
    "3. Look at how the activation of the target neuron reacts to those feature visualizations over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Container, Tuple, List, Dict, TypedDict\n",
    "from dataclasses import asdict\n",
    "import math\n",
    "from typing import Callable\n",
    "import functools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb \n",
    "from torch.optim.lr_scheduler import LambdaLR \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "from devinterp.config import Config, OptimizerConfig, SchedulerConfig\n",
    "from devinterp.checkpoints import CheckpointManager\n",
    "from devinterp.logging import Logger\n",
    "from devinterp.learner import Learner\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/paperspace/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/paperspace/Projects/devinterp/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paperspace/Projects/devinterp/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transforms)\n",
    "test_set = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 128\n",
      "device: cuda\n",
      "entity: devinterp\n",
      "num_epochs: 165\n",
      "num_steps: 64000\n",
      "num_training_samples: 50000\n",
      "optimizer_config:\n",
      "  lr: 0.1\n",
      "  momentum: 0.9\n",
      "  optimizer_type: SGD\n",
      "  weight_decay: 0.0001\n",
      "project: resnet18\n",
      "scheduler_config:\n",
      "  gamma: 0.5\n",
      "  last_epoch: -1\n",
      "  milestones:\n",
      "  - 16000\n",
      "  - 32000\n",
      "  - 48000\n",
      "  scheduler_type: MultiStepLR\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/Projects/devinterp/devinterp/utils.py:45: UserWarning: Number of steps in int_logspace is not 100, got 88.\n",
      "  warnings.warn(f\"Number of steps in int_logspace is not {num}, got {len(result)}.\")\n",
      "/home/paperspace/Projects/devinterp/devinterp/utils.py:45: UserWarning: Number of steps in int_logspace is not 25, got 24.\n",
      "  warnings.warn(f\"Number of steps in int_logspace is not {num}, got {len(result)}.\")\n"
     ]
    }
   ],
   "source": [
    "from devinterp.config import Config\n",
    "import yaml\n",
    "\n",
    "config = Config(\n",
    "    num_training_samples=len(train_set), \n",
    "    num_steps=64_000, \n",
    "    project=\"resnet18\", \n",
    "    entity=\"devinterp\", \n",
    "    logging_steps=(100, 100), \n",
    "    checkpoint_steps=(25, 25),\n",
    "    optimizer_config=OptimizerConfig(\n",
    "        optimizer_type=\"SGD\",\n",
    "        lr=0.1,\n",
    "        momentum=0.9,\n",
    "    ),\n",
    "    scheduler_config=SchedulerConfig(\n",
    "        scheduler_type=\"MultiStepLR\",\n",
    "        milestones=[16_000, 32_000, 48_000], \n",
    "        gamma=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(yaml.dump(config.model_dump(exclude=(\"logging_steps\", \"checkpoint_steps\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4uq1n32b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch/Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch/Loss</td><td>7.21299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-cloud-36</strong> at: <a href='https://wandb.ai/devinterp/resnet18/runs/4uq1n32b' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/4uq1n32b</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230815_120111-4uq1n32b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4uq1n32b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407c15176c5641a6b38e66400c3b7423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671020416682343, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/Projects/devinterp/notebooks/wandb/run-20230815_120151-kcc2oc13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devinterp/resnet18/runs/kcc2oc13' target=\"_blank\">deep-mountain-37</a></strong> to <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devinterp/resnet18' target=\"_blank\">https://wandb.ai/devinterp/resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devinterp/resnet18/runs/kcc2oc13' target=\"_blank\">https://wandb.ai/devinterp/resnet18/runs/kcc2oc13</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a58d412c034c69825e11c1b72dffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 Batch 0/64000 Loss: ?.??????:   0%|          | 0/64000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss_metric(model, data, target, output):\n",
    "    return F.cross_entropy(output, target, reduction=\"sum\")\n",
    "\n",
    "def accuracy_metric(model, data, target, output):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    return pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "def eval_learner(learner: Learner):\n",
    "    return dataloaders_reduce(learner.model, {\"Train\": learner.train_loaders, \"Test\": learner.test_loaders}, {\"Loss\": loss_metric, \"Accuracy\": accuracy_metric}, device=learner.config.device)\n",
    "\n",
    "learner = Learner(model, train_set, test_set, config, metrics=[eval_learner])\n",
    "learner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature visualization\n",
    "\n",
    "We have a trained `model` (and a bunch of checkpoints). First, let's do some classic feature visualization on the final network. We'll select a few random neurons from ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = CheckpointManager('resnet18/cifar10', 'devinterp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.load_state_dict(checkpoints[-1])\n",
    "# model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# model: nn.Module = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v1', pretrained=True)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"\n",
    "    With this version, you can use the ActivationExtractor with the following location formats:\n",
    "\n",
    "    - 'layer1.0.conv1.weight.3': Only channel is specified; y and x default to center.\n",
    "    - 'layer1.0.conv1.weight.3.2': Channel and y are specified; x defaults to center.\n",
    "    - 'layer1.0.conv1.weight.3..2': Channel and x are specified; y defaults to center.\n",
    "    - 'layer1.0.conv1.weight.3.2.2': Channel, y, and x are all specified.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, location):\n",
    "        self.activation = None\n",
    "        self.model = model\n",
    "        self.location = location.split('.')\n",
    "        self.layer_path = []\n",
    "        self.channel = None\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "\n",
    "        # Split the location into layer path and neuron indices\n",
    "        state_dict_keys = list(model.state_dict().keys())\n",
    "        for part in self.location:\n",
    "            self.layer_path.append(part)\n",
    "            path = '.'.join(self.layer_path)\n",
    "            \n",
    "            if any(key.startswith(path) for key in state_dict_keys):\n",
    "                continue\n",
    "            else:\n",
    "                self.layer_path.pop()\n",
    "                self.channel, *yx = map(int, self.location[len(self.layer_path):])\n",
    "                if yx:\n",
    "                    self.y = yx[0]\n",
    "                    if len(yx) > 1:\n",
    "                        self.x = yx[1]\n",
    "                break\n",
    "\n",
    "        # Get the target layer\n",
    "        self.layer = model\n",
    "        for part in self.layer_path[:-1]:\n",
    "            self.layer = getattr(self.layer, part)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        y = self.y if self.y is not None else output.size(2) // 2\n",
    "        x = self.x if self.x is not None else output.size(3) // 2\n",
    "\n",
    "        self.activation = output[0, self.channel, y, x]\n",
    "\n",
    "    def register_hook(self):\n",
    "        handle = self.layer.register_forward_hook(self.hook_fn)\n",
    "        return handle\n",
    "\n",
    "\n",
    "def gen_image(image: torch.Tensor):\n",
    "    # Process the optimized input\n",
    "    image = image.detach().cpu().squeeze(0)\n",
    "\n",
    "    image -= image.min()\n",
    "    image /= image.max()\n",
    "\n",
    "    # Create grid\n",
    "    grid_image = vutils.make_grid([image], nrow=1)\n",
    "\n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_image_np = grid_image.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "\n",
    "    return grid_image_np\n",
    "\n",
    "def show_image(image: torch.Tensor):\n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_image_np = gen_image(image)\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(5, 5))  # You can change the size as you prefer\n",
    "    plt.imshow(grid_image_np)\n",
    "    plt.axis('off') # to remove the axis\n",
    "    plt.show()\n",
    "\n",
    "def show_images(*images: torch.Tensor, nrow=None, **kwargs):\n",
    "    # Normalize images to [0,1] and create grid\n",
    "    images = [img - img.min() for img in images]\n",
    "    images = [img / img.max() for img in images]\n",
    "    images = [img.squeeze(0) for img in images]\n",
    "    \n",
    "    # Create grid\n",
    "    grid_image = vutils.make_grid(images, nrow=nrow or len(images))\n",
    "\n",
    "    # Convert to numpy and transpose for plotting\n",
    "    grid_image_np = grid_image.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(15, 30), **kwargs)  # You can change the size as you prefer\n",
    "    plt.imshow(grid_image_np)\n",
    "    plt.axis('off') # to remove the axis\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_jitter(input_image, jitter_amount=2):\n",
    "    \"\"\"Applies jitter by randomly shifting the image.\"\"\"\n",
    "    if not jitter_amount:\n",
    "        return input_image\n",
    "\n",
    "    x_shift, y_shift = torch.randint(jitter_amount, -jitter_amount, (2,))\n",
    "    return torch.roll(input_image, shifts=(x_shift, y_shift), dims=(2, 3))\n",
    "\n",
    "def render(model: nn.Module, location: str, thresholds: list[int]=[512], verbose: bool = True, seed: int = 0, device: str = torch.device) -> tuple[list[torch.Tensor], float]:\n",
    "    # Assuming 'model' is your pre-trained ResNet model and 'location' is the string specifying the neuron's location\n",
    "    model.to(device)\n",
    "    model = model.eval()\n",
    "    extractor = ActivationExtractor(model, location)\n",
    "    handle = extractor.register_hook()\n",
    "\n",
    "    # Create a random image (1x3x224x224) to start optimization, with same size as typical ResNet input\n",
    "    torch.manual_seed(seed)\n",
    "    input_image = torch.rand((1, 3, 32, 32), requires_grad=True, device=device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam([input_image], lr=0.01, weight_decay=1e-3)\n",
    "    jitter_amount = 0\n",
    "\n",
    "    final_images = []\n",
    "\n",
    "    # Optimization loop\n",
    "    pbar = range(max(thresholds) + 1)\n",
    "\n",
    "    if verbose:\n",
    "        pbar = tqdm(pbar, desc=f\"Visualizing {location} (activation: ???)\")\n",
    "\n",
    "    for iteration in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        model(input_image)  # Forward pass through the model to trigger the hook\n",
    "        activation = extractor.activation\n",
    "        loss = -activation  # Maximizing activation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        input_image.data = add_jitter(input_image.data.detach().clone(), jitter_amount=-jitter_amount)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description(f\"Visualizing {location} (activation: {activation.item():.2f})\")\n",
    "\n",
    "        if iteration in thresholds:\n",
    "            # if verbose:\n",
    "            #     show_image(input_image)\n",
    "\n",
    "            image = input_image.detach().clone()\n",
    "            image = torch.reshape(image, (1, 3, 32, 32))            \n",
    "            final_images.append(image)\n",
    "\n",
    "    handle.remove()  # Remove the hook after the loop\n",
    "\n",
    "    return final_images, extractor.activation.item()\n",
    "\n",
    "\n",
    "def render_multiple(model: nn.Module, *locations: str, thresholds: list[int]=[512], verbose: bool = True, init_seed: int = 0, device: str = \"cuda\", **kwargs) -> list[tuple[list[torch.Tensor], float]]:\n",
    "    results = []\n",
    "\n",
    "    for i, location in enumerate(locations):\n",
    "        images, activation = render(\n",
    "            model, \n",
    "            location = location,\n",
    "            thresholds = thresholds,\n",
    "            verbose = verbose,\n",
    "            seed=init_seed + i,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        if verbose: \n",
    "            show_images(*images, **kwargs)\n",
    "\n",
    "        results.append((images, activation))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = render_multiple(\n",
    "    model,    \n",
    "    'layer1.0.conv1.weight.0',\n",
    "    'layer1.0.conv2.weight.1',\n",
    "    'layer1.1.conv1.weight.7',\n",
    "    'layer1.1.conv2.weight.4',\n",
    "    'layer2.0.conv1.weight.3',\n",
    "    'layer2.0.conv2.weight.2',\n",
    "    thresholds=[0, 64, 128, 256, 512],\n",
    "    verbose=False,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "show_images(*[images[-1] for (images, _) in results], dpi=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximally active neurons\n",
    "\n",
    "Let's go through all neurons in the model and rank them by their activation. We will then plot the top 10 most active neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "def gen_conv_neurons(model: nn.Module):\n",
    "    \"\"\"Generate convolutional neurons from a PyTorch model.\"\"\"\n",
    "    channel_locations = []\n",
    "\n",
    "    def recursive_search(module, prefix):\n",
    "        for name, submodule in module.named_children():\n",
    "            path = prefix + '.' + name if prefix else name\n",
    "\n",
    "            # Check if the submodule is a convolutional layer\n",
    "            if isinstance(submodule, Conv2d):\n",
    "                # Generate locations for all channels in this convolutional layer\n",
    "                for channel in range(submodule.out_channels):\n",
    "                    location = f\"{path}.weight.{channel}\"\n",
    "                    channel_locations.append(location)\n",
    "\n",
    "            # Recursively search through children\n",
    "            recursive_search(submodule, path)\n",
    "\n",
    "    recursive_search(model, '')\n",
    "\n",
    "    return channel_locations\n",
    "\n",
    "conv_neurons = gen_conv_neurons(model)[64:]\n",
    "print(conv_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_results =  []\n",
    "\n",
    "for i in range(0, len(conv_neurons), 10):\n",
    "    section = conv_neurons[i:i+10]\n",
    "    print(section)\n",
    "    _results = render_multiple(model, *section, thresholds=[256], device=\"cuda:0\", verbose=False)\n",
    "    show_images(*[images[-1] for (images, _) in _results], dpi=50)\n",
    "    neurons_results.extend(_results)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Saving results at {i}\")\n",
    "        torch.save(neurons_results, \"../visualizations/restnet-cifar10.pt\")\n",
    "        \n",
    "        # Print the 100 most activated neurons\n",
    "        print([(name, activation) for (name, (_, activation)) in sorted(zip(conv_neurons, neurons_results), key=lambda x: x[1][1], reverse=True)[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developmental analysis of a sample neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neuron = \"layer1.1.conv1.weights.7\"\n",
    "viz, activation = render(model, sample_neuron, seed=0)[-1]\n",
    "print(activation)\n",
    "show_image(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(checkpoints, desc=\"Looping checkpoints (activation: ???)\")\n",
    "activations = []\n",
    "\n",
    "for state_dict in pbar:\n",
    "    model.load_state_dict(state_dict)\n",
    "    extractor = ActivationExtractor(model, sample_neuron)\n",
    "    handle = extractor.register_hook()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model(viz) \n",
    "        activations.append(extractor.activation)\n",
    "    \n",
    "    pbar.set_description(f\"Looping checkpoints (activation: {extractor.activation.item():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([b for (_, b) in checkpoints.checkpoints][-5:], activations[-5:])\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do feature visualization at the very start, at 90  steps (where it reaches a minimum) at 5k steps where it's close to 0, at 8600, at 9000, and at the last step. \n",
    "\n",
    "# First let's get the closest checkpoints to these steps\n",
    "\n",
    "ideal_checkpoint_steps = [90, 5000, 8600, 9000, 9999]\n",
    "\n",
    "def get_closest_checkpoint(checkpoints: list[tuple[int, int]], step: int) -> int:\n",
    "    return min([chkpt for chkpt in checkpoints], key=lambda x: abs(x[1] - step))\n",
    "\n",
    "checkpoint_steps = [get_closest_checkpoint(checkpoints.checkpoints, step) for step in ideal_checkpoint_steps]\n",
    "checkpoint_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (epoch, batch_idx) in tqdm(checkpoint_steps, desc=\"Going through checkpoints\"):\n",
    "    model.load_state_dict(checkpoints[(epoch, batch_idx)])\n",
    "    vizs, activation = render(model, sample_neuron, seed=0, thresholds=[0, 64, 128, 256, 512], verbose=True)\n",
    "    show_images(*vizs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a whole set of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_results = torch.load(\"../visualizations/restnet-cifar10.pt\", map_location=torch.device('cpu'))\n",
    "viz_results = sorted([(name, a, img) for (img, a), name in zip(viz_results, conv_neurons)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "eps = 1e-4\n",
    "large_eps = 100\n",
    "\n",
    "activations = [a for _, a, _ in viz_results]\n",
    "\n",
    "print(\"# Negative activations: \", len([a for a in activations if a < 0]))\n",
    "print(\"# Zero activations: \", len([a for a in activations if a == 0]))\n",
    "print(\"# Insignificant positive activations: \", len([a for a in activations if 0 < a <= eps]))\n",
    "print(\"# Moderate positive activations: \", len([a for a in activations if eps < a <= large_eps]))\n",
    "print(\"# Large positive activations: \", len([a for a in activations if large_eps < a]))\n",
    "\n",
    "activations = [a for a in activations if a > eps]\n",
    "\n",
    "log_bins = np.logspace(np.log10(min(activations)),\n",
    "                       np.log10(max(activations)), num=10)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(activations, bins=log_bins)\n",
    "plt.xscale('log') # Optional, if you want the x-axis to be logarithmic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 5 from each category by random\n",
    "np.random.seed(2)\n",
    "\n",
    "sample_neurons = [\n",
    "    *np.random.choice([n for n, a, _ in viz_results if a < -eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if -eps <= a <= eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if eps < a <= large_eps], size=5, replace=False),\n",
    "    *np.random.choice([n for n, a, _ in viz_results if large_eps < a], size=5, replace=False),\n",
    "]\n",
    "print(sample_neurons)\n",
    "images = [imgs[-1] for n, _, imgs in viz_results if n in sample_neurons]\n",
    "show_images(\n",
    "    *images,\n",
    "    nrow=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_multiple(model: nn.Module, checkpoints: CheckpointManager, *locations: str, opt_steps: int = 512, **kwargs):\n",
    "    model.load_state_dict(checkpoints[-1])\n",
    "    model.eval()\n",
    "\n",
    "    final_vizs: dict[str, torch.Tensor] = {}\n",
    "    vizs: dict[str, list[torch.Tensor]] = {}\n",
    "    activations: dict[str, list[float]] = {}\n",
    "   \n",
    "    # Create the visualizations for the last checkpoint\n",
    "    for location, _location_vizs in zip(locations, tqdm(render_multiple(model, *locations, thresholds=[opt_steps], **kwargs), desc=\"Creating initial visualizations\")):\n",
    "        final_vizs[location] = _location_vizs[0][0]\n",
    "        vizs[location] = []\n",
    "        activations[location] = []\n",
    " \n",
    "    for i, state_dict in enumerate(tqdm(checkpoints, desc=\"Visiting checkpoints\")):\n",
    "        batch_idx = checkpoints.checkpoints[i][1]\n",
    "\n",
    "        # Render the visualization for the next checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        for location in locations:\n",
    "            viz = final_vizs[location]\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "            extractor = ActivationExtractor(model, location)\n",
    "            handle = extractor.register_hook()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model(viz) \n",
    "                activations[location].append(extractor.activation.item())\n",
    "\n",
    "            handle.remove()\n",
    "\n",
    "        wandb.log({f\"Activations/{location}\": activations[location][-1] for location in locations}, step=batch_idx, commit=False)\n",
    "            \n",
    "        # Visualize this checkpoint\n",
    "        if i % 20 or i == len(checkpoints) - 1:\n",
    "            for location, _location_vizs in zip(locations, tqdm(render_multiple(model, *locations, thresholds=[opt_steps], **kwargs), desc=f\"Creating visualizations for batch {batch_idx}\")):\n",
    "                viz = _location_vizs[0][0]\n",
    "                vizs[location].append(viz)\n",
    "                image_np = gen_image(viz)\n",
    "                image = wandb.Image(image_np, caption=f\"Optimized {location} at batch {batch_idx}\")\n",
    "\n",
    "                wandb.log({f\"Visualizations/{location}\": image}, step=batch_idx)\n",
    "\n",
    "    return vizs, activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()\n",
    "# run_id = input(\"Run ID: \")\n",
    "wandb.init(project=config.project, entity=config.entity)\n",
    "results = evolve_multiple(model, checkpoints, *sample_neurons, device=\"cpu\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['layer3.0.conv2.weight.206', 'layer3.0.conv1.weight.149', 'layer3.0.conv1.weight.118', 'layer3.0.conv2.weight.63', 'layer2.0.conv1.weight.102', 'layer2.0.conv2.weight.110', 'layer2.0.conv1.weight.15', 'layer3.1.conv1.weight.20', 'layer2.0.conv2.weight.19', 'layer3.0.conv1.weight.205', 'layer1.0.conv2.weight.54', 'layer2.0.conv2.weight.12', 'layer2.0.downsample.0.weight.99', 'layer1.0.conv2.weight.0', 'layer1.0.conv1.weight.47', 'layer1.0.conv2.weight.41', 'layer1.0.conv2.weight.51', 'layer3.0.downsample.0.weight.125', 'layer2.0.conv1.weight.108', 'layer1.1.conv2.weight.32']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
