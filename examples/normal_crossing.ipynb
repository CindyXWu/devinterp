{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Crossing RLCT Estimation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/normal_crossing.ipynb)\n",
    "\n",
    "This notebook measures the RLCT of normal crossings using two algorithms: SGNHT (Stochastic Gradient Nose-Hoover Thermostat) and SGLD (Stochastic Gradient Langevin Dynamics). The target model is a polynomial model characterized by $w_1^a * w_2^b$ for some $(a, b)$, where $w_1$ and $w_2$ are weights to be learned. The data is generated with gaussian noise around the origin, so the model achieves its lowest loss when $w_1=0$ or $w_2 =0$. We first estimate the RLCTs from different points of weight space. Then, we plot some sampled trajectories, and do a quick reproduction of Table 1 of [Lau et al. (2023)](https://arxiv.org/pdf/2308.12108.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install devinterp matplotlib pyro-ppl seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.optim.sgd import SGD\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.optim.sgld_ma import SGLD_MA\n",
    "from devinterp.optim.sgnht import SGNHT\n",
    "from devinterp.slt import estimate_learning_coeff, sample, estimate_learning_coeff_with_summary\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# plotting\n",
    "CMAP = sns.color_palette(\"muted\", as_cmap=True)\n",
    "PRIMARY, SECONDARY, TERTIARY = CMAP[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "SIGMA = 0.25\n",
    "NUM_TRAIN_SAMPLES = 1000\n",
    "BATCH_SIZE = NUM_TRAIN_SAMPLES\n",
    "CRITERION = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some necessary functions\n",
    "class PolyModel(nn.Module):\n",
    "    def __init__(self, powers):\n",
    "        super(PolyModel, self).__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.tensor([1.0, 0.3], dtype=torch.float32, requires_grad=True)\n",
    "        )\n",
    "        self.powers = powers\n",
    "\n",
    "    def forward(self, x):\n",
    "        multiplied = torch.prod(self.weights**self.powers)\n",
    "        x = x * multiplied\n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_dataset_for_seed(seed=0):\n",
    "    x = torch.normal(0, 2, size=(NUM_TRAIN_SAMPLES,))\n",
    "    y = SIGMA * torch.normal(0, 1, size=(NUM_TRAIN_SAMPLES,))\n",
    "    train_data = TensorDataset(x, y)\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return train_loader, train_data, x, y\n",
    "\n",
    "\n",
    "def estimate_rlct_helper(model, train_loader, criterion, optimizer_kwargs, sampling_method, device):\n",
    "    return estimate_learning_coeff(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        sampling_method=sampling_method,\n",
    "        num_chains=5,\n",
    "        num_draws=1000,\n",
    "        num_burnin_steps=0,\n",
    "        num_steps_bw_draws=1,\n",
    "        verbose=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "def estimate_rlct_with_accept_ratio_helper(model, train_loader, criterion, optimizer_kwargs, sampling_method, device):\n",
    "    data_dict = estimate_learning_coeff_with_summary(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        sampling_method=sampling_method,\n",
    "        num_chains=5,\n",
    "        num_draws=1000,\n",
    "        num_burnin_steps=0,\n",
    "        num_steps_bw_draws=1,\n",
    "        verbose=True,\n",
    "        device=device,\n",
    "    )\n",
    "    return data_dict[\"mean\"], data_dict[\"accept_ratio\"]\n",
    "\n",
    "\n",
    "def get_rlcts(\n",
    "    train_loader,\n",
    "    train_data,\n",
    "    weights=[0.0, 0.0],\n",
    "    lr=0.0005,\n",
    "    powers=torch.tensor([1, 2]).to(DEVICE),\n",
    "    n_estimates=10,\n",
    "    return_results_dict=True,\n",
    "    show_accept_ratio=False,\n",
    "):\n",
    "    model = PolyModel(powers).to(DEVICE)\n",
    "    model.weights = nn.Parameter(torch.tensor(weights, dtype=torch.float32, requires_grad=True))\n",
    "    rlct_estimates_sgnht, rlct_estimates_sgld = [], []\n",
    "\n",
    "    sgnht_kwargs = {\n",
    "        \"lr\": lr,\n",
    "        \"diffusion_factor\": 0.01,\n",
    "        \"bounding_box_size\": 0.5,\n",
    "        \"num_samples\": len(train_data),\n",
    "    }\n",
    "    sgld_kwargs = {\n",
    "        \"lr\": lr,\n",
    "        \"elasticity\": 1.0,\n",
    "        \"temperature\": \"adaptive\",\n",
    "        \"num_samples\": len(train_data),\n",
    "    }\n",
    "    for _ in range(n_estimates):\n",
    "        if show_accept_ratio:\n",
    "            rlct_sgnht, accept_ratio_sgnht = estimate_rlct_with_accept_ratio_helper(\n",
    "                model, train_loader, CRITERION, sgnht_kwargs, SGNHT, DEVICE\n",
    "            )\n",
    "            rlct_sgld, accept_ratio_sgld = estimate_rlct_with_accept_ratio_helper(\n",
    "                model, train_loader, CRITERION, sgld_kwargs, SGLD_MA, DEVICE\n",
    "            )\n",
    "        else:\n",
    "            accept_ratio_sgnht, accept_ratio_sgld = None, None\n",
    "            rlct_sgnht = estimate_rlct_helper(\n",
    "                model, train_loader, CRITERION, sgnht_kwargs, SGNHT, DEVICE\n",
    "            )\n",
    "            rlct_sgld = estimate_rlct_helper(model, train_loader, CRITERION, sgld_kwargs, SGLD_MA, DEVICE)\n",
    "        if not math.isnan(rlct_sgnht):\n",
    "            rlct_estimates_sgnht.append(rlct_sgnht)\n",
    "        if not math.isnan(rlct_sgld):\n",
    "            rlct_estimates_sgld.append(rlct_sgld)\n",
    "\n",
    "    if return_results_dict:\n",
    "        results_dict = {\n",
    "            \"weights\": weights,\n",
    "            \"lr\": lr,\n",
    "            \"powers\": powers,\n",
    "            \"n_estimates\": n_estimates,\n",
    "            \"rlct_estimates_sgnht\": rlct_estimates_sgnht,\n",
    "            \"rlct_estimates_sgld\": rlct_estimates_sgld,\n",
    "        }\n",
    "        if show_accept_ratio:\n",
    "            results_dict[\"accept_ratio_sgnht\"] = accept_ratio_sgnht\n",
    "            results_dict[\"accept_ratio_sgld\"] = accept_ratio_sgld\n",
    "        return results_dict\n",
    "    else:\n",
    "        return rlct_estimates_sgnht, rlct_estimates_sgld\n",
    "\n",
    "\n",
    "def plot_rlcts(estimated_rlcts, sample_points, actual_rlcts):\n",
    "    fig, axes = plt.subplots(1, len(estimated_rlcts), figsize=(14, 3))\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    for i, (rlct_estimates_sgnht, rlct_estimates_sgld) in enumerate(estimated_rlcts):\n",
    "        axes[i].set_xlim([-0.5, 1.5])\n",
    "        axes[i].set_title(rf\"$w_1={sample_points[i][0]:.1f}, w_2={sample_points[i][1]:.1f}$\")\n",
    "        axes[i].hist(\n",
    "            rlct_estimates_sgnht,\n",
    "            alpha=0.6,\n",
    "            bins=20,\n",
    "            range=(-0.5, 1.5),\n",
    "            label=\"SGNHT\",\n",
    "            color=PRIMARY,\n",
    "        )\n",
    "        axes[i].hist(\n",
    "            rlct_estimates_sgld,\n",
    "            alpha=0.6,\n",
    "            bins=20,\n",
    "            range=(-0.5, 1.5),\n",
    "            label=\"SGLD\",\n",
    "            color=SECONDARY,\n",
    "        )\n",
    "        axes[i].axvline(actual_rlcts[i], color=\"black\", linestyle=\"--\")\n",
    "        axes[i].set_xlabel(r\"$\\hat\\lambda$\")\n",
    "    axes[0].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "First, let's estimate some RLCTs of a simple normal crossing for $(a, b) = (1, 2)$. We first estimate the RLCTs at the origin, near the origin, and at points where either $w_1=0$ or $w_2=0$ (but not both). This is similar to Figure 1 from [Lau et al. (2023)](https://arxiv.org/pdf/2308.12108.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_data, x, y = generate_dataset_for_seed(0)\n",
    "\n",
    "sample_points = [[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.1, 0.1]]\n",
    "actual_rlcts = [0.25, 0.25, 0.5, 0.25]\n",
    "estimated_rlcts = [\n",
    "    get_rlcts(\n",
    "        train_loader, train_data, weights=sample_point, n_estimates=20, return_results_dict=False\n",
    "    )\n",
    "    for sample_point in sample_points\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rlcts(estimated_rlcts, sample_points, actual_rlcts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Plots\n",
    "\n",
    "Let's see how well our different sampling methods explore the local loss landscape. We start out sampling at $w_1 = 0.5, w_2 = 0.01$, and plot the resulting trajectories after 10k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "def hex_to_rgb(hex_color):\n",
    "    hex_color = hex_color.lstrip(\"#\")\n",
    "    return tuple(int(hex_color[i : i + 2], 16) / 255.0 for i in (0, 2, 4))\n",
    "\n",
    "\n",
    "lighter_factor = 0.9  # Between 0 and 1, higher values make it closer to white\n",
    "lighter_SECONDARY = tuple([x + (1 - x) * lighter_factor for x in hex_to_rgb(SECONDARY)[:3]] + [1.0])\n",
    "\n",
    "colors = [SECONDARY, lighter_SECONDARY]\n",
    "n_bins = 20  # Number of bins\n",
    "contour_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=n_bins)\n",
    "\n",
    "powers = torch.tensor([1, 2]).to(DEVICE)\n",
    "model = PolyModel(powers).to(DEVICE)\n",
    "\n",
    "\n",
    "def plot_trajectories(trajectories, names, device=DEVICE):\n",
    "    \"\"\"Sampled w1, w2 values are plotted on the contour plot of the loss function.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(trajectories), figsize=(14, 6))\n",
    "    w1_range = np.linspace(-2, 2, 21)\n",
    "    w2_range = np.linspace(-2, 2, 21)\n",
    "    w1_vals, w2_vals = np.meshgrid(w1_range, w2_range)\n",
    "    Z = np.zeros_like(w1_vals, dtype=float)\n",
    "\n",
    "    for i in range(w1_vals.shape[0]):\n",
    "        for j in range(w1_vals.shape[1]):\n",
    "            w1 = w1_vals[i, j]\n",
    "            w2 = w2_vals[i, j]\n",
    "            model.weights = nn.Parameter(torch.tensor([w1, w2], dtype=torch.float32).to(device))\n",
    "            Z[i, j] = (\n",
    "                model.to(device)(torch.tensor(1.0).to(device)).item() ** 2\n",
    "            )  # MSE, so square this\n",
    "\n",
    "    custom_levels = np.linspace(Z.min(), Z.max() * 0.04, n_bins)\n",
    "\n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        axes[i].contourf(w1_vals, w2_vals, Z, levels=custom_levels, cmap=contour_cmap, alpha=0.8)\n",
    "        weights = trajectory[\"model_weights\"]\n",
    "        draws_array = np.array(\n",
    "            [\n",
    "                d.detach().cpu().numpy()\n",
    "                for d in weights\n",
    "                if w1_range[0] <= d[0] <= w1_range[-1] and w2_range[0] <= d[1] <= w2_range[-1]\n",
    "            ]\n",
    "        )\n",
    "        sns.scatterplot(\n",
    "            x=draws_array[:, 0],\n",
    "            y=draws_array[:, 1],\n",
    "            marker=\"x\",\n",
    "            ax=axes[i],\n",
    "            s=10,\n",
    "            color=PRIMARY,\n",
    "        )\n",
    "        axes[i].axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "        axes[i].axvline(0, linestyle=\"--\", color=\"gray\")\n",
    "        axes[i].set_xlabel(r\"$w_{1}$\")\n",
    "        axes[i].set_ylabel(r\"$w_{2}$\")\n",
    "        axes[i].set_title(names[i])\n",
    "        axes[i].grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_data, x, y = generate_dataset_for_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "model.weights = nn.Parameter(\n",
    "    torch.tensor([0.5, 0.01], dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    ")\n",
    "lr = 0.0003\n",
    "trace_sgnht = sample(\n",
    "    model,\n",
    "    train_loader,\n",
    "    criterion=CRITERION,\n",
    "    optimizer_kwargs=dict(\n",
    "        lr=lr,\n",
    "        diffusion_factor=0.01,\n",
    "        num_samples=len(train_data),\n",
    "    ),\n",
    "    sampling_method=SGNHT,\n",
    "    num_chains=1,\n",
    "    num_draws=10_000,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_bw_draws=1,\n",
    "    verbose=False,\n",
    "    return_weights=True,\n",
    "    device=DEVICE,\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "model.weights = nn.Parameter(\n",
    "    torch.tensor([0.5, 0.01], dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    ")\n",
    "trace_sgld = sample(\n",
    "    model,\n",
    "    train_loader,\n",
    "    criterion=CRITERION,\n",
    "    optimizer_kwargs=dict(\n",
    "        lr=lr,\n",
    "        elasticity=1.0,\n",
    "        temperature=\"adaptive\",\n",
    "        num_samples=len(train_data),\n",
    "    ),\n",
    "    sampling_method=SGLD,\n",
    "    num_chains=1,\n",
    "    num_draws=10_000,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_bw_draws=1,\n",
    "    verbose=False,\n",
    "    return_weights=True,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories([trace_sgld, trace_sgnht], names=[\"SGLD\", \"SGNHT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "model.weights = nn.Parameter(\n",
    "    torch.tensor([0.5, 0.01], dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "model.powers = powers.to(DEVICE)\n",
    "\n",
    "trace_sgd = sample(\n",
    "    model,\n",
    "    train_loader,\n",
    "    criterion=CRITERION,\n",
    "    optimizer_kwargs=dict(lr=1_000 * lr),\n",
    "    sampling_method=SGD,\n",
    "    num_chains=1,\n",
    "    num_draws=10_000,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_bw_draws=1,\n",
    "    verbose=False,\n",
    "    return_weights=True,\n",
    "    device=DEVICE,\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "model.powers = powers.to(\"cpu\")\n",
    "# Define a Pyro model for Bayesian inference\n",
    "w_prior = dist.Normal(model.weights, torch.ones_like(model.weights)).to_event(1)\n",
    "\n",
    "\n",
    "def pyro_model(x, y):\n",
    "    w = pyro.sample(\"weights\", w_prior)\n",
    "    model.weights = nn.Parameter(w.to(\"cpu\"))\n",
    "    y_pred = model(x.to(\"cpu\"))\n",
    "    pyro.sample(\"obs\", dist.Normal(y_pred, 0.5), obs=y.to(\"cpu\"))\n",
    "\n",
    "\n",
    "# Perform MCMC sampling using NUTS\n",
    "nuts_kernel = NUTS(pyro_model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=20)\n",
    "mcmc.run(x, y)\n",
    "\n",
    "\n",
    "# Get posterior samples\n",
    "posterior_samples = mcmc.get_samples()\n",
    "trace_mcmc = {\"model_weights\": posterior_samples[\"weights\"]}\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.powers = powers.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories([trace_mcmc, trace_sgd], names=[\"MCMC-NUTS\", \"SGD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Finally, let's check if we get the expected lambdahat values for a few different crossings. This is similar to Table 1 from [Lau et al. (2023)](https://arxiv.org/pdf/2308.12108.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 10\n",
    "SHOW_ACCEPT_RATIO = True\n",
    "\n",
    "datasets = [generate_dataset_for_seed(seed) for seed in range(n_seeds)]\n",
    "powers_to_test = [\n",
    "    torch.tensor([1, 3]).to(DEVICE),\n",
    "    torch.tensor([1, 2]).to(DEVICE),\n",
    "    torch.tensor([0, 1]).to(DEVICE),\n",
    "    torch.tensor([1, 0]).to(DEVICE),\n",
    "    torch.tensor([1, 1]).to(DEVICE),\n",
    "]\n",
    "rlct_estimates = [\n",
    "    get_rlcts(\n",
    "        train_loader,\n",
    "        train_data,\n",
    "        lr=lr,\n",
    "        powers=powers,\n",
    "        n_estimates=1,\n",
    "        return_results_dict=True,\n",
    "        show_accept_ratio=SHOW_ACCEPT_RATIO,\n",
    "    )\n",
    "    for (train_loader, train_data, _, _) in datasets\n",
    "    for powers in powers_to_test\n",
    "    for lr in [0.0005, 0.001, 0.003]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(rlct_estimates)\n",
    "df.drop(\"weights\", axis=1, inplace=True)\n",
    "df.drop(\"n_estimates\", axis=1, inplace=True)\n",
    "df[\"powers\"] = df[\"powers\"].apply(lambda x: tuple([tensor.tolist() for tensor in x]))\n",
    "df[\"rlct_estimates_sgnht\"] = df[\"rlct_estimates_sgnht\"].apply(lambda x: x[0] if len(x) == 1 else x)\n",
    "df[\"rlct_estimates_sgld\"] = df[\"rlct_estimates_sgld\"].apply(lambda x: x[0] if len(x) == 1 else x)\n",
    "if SHOW_ACCEPT_RATIO:\n",
    "    df[\"accept_ratio_sgnht\"] = df[\"accept_ratio_sgnht\"].apply(lambda x: x[0] if len(x) == 1 else x)\n",
    "    df[\"accept_ratio_sgld\"] = df[\"accept_ratio_sgld\"].apply(lambda x: x[0] if len(x) == 1 else x)\n",
    "    grouped = df.groupby([\"lr\", \"powers\"])[[\"rlct_estimates_sgnht\", \"accept_ratio_sgnht\", \"rlct_estimates_sgld\", \"accept_ratio_sgld\"]].agg(\n",
    "        [\"mean\", \"std\"]\n",
    "    )\n",
    "else:\n",
    "    grouped = df.groupby([\"lr\", \"powers\"])[[\"rlct_estimates_sgnht\", \"rlct_estimates_sgld\"]].agg(\n",
    "        [\"mean\", \"std\"]\n",
    "    )\n",
    "print(grouped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
