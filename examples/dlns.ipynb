{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Deep Linear Networks\n",
            "\n",
            "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/dlns.ipynb)\n",
            "\n",
            "Here, we repeat the experiments by [Jacot et al. (2022)](https://arxiv.org/abs/2106.15933), then [estimate SLT-derived invariants like the learning coefficient](https://github.com/edmundlth/scalable_learning_coefficient_with_sgld/blob/v1.0/experiment.py).\n",
            "\n",
            "Currently, this only looks at the learning task behind figure 3 (not the MC loss behind figure 2).\n",
            "\n",
            "A **deep linear network** (DLN) of length $L$ is a neural network with $L$ layers of widths $n_0, \\dots, n_L$, that computes the transformation:\n",
            "\n",
            "$$\n",
            "\\begin{align}\n",
            "f: \\mathbb{R}^{n_0} &\\to \\mathbb{R}^{n_L} \\\\\n",
            "x &\\mapsto W_L \\cdots W_1 x =: A_\\theta x,\n",
            "\\end{align}\n",
            "$$\n",
            "\n",
            "Parametrized by $\\theta \\in \\mathbb{R}^P$, where $P = \\sum_{l=1}^L n_{l-1} n_l$ is the number of parameters.\n",
            "\n",
            "For convenience, we consider **rectangular networks**, or $(L, w)$-DLNs, with constant hidden width $w$ across all layers: $n_1 = \\dots = n_{L-1} = w$.\n",
            "\n",
            "## Hyperparameters\n",
            "\n",
            "- $L$ is the number of layers\n",
            "- $N=n_0$ is the input dimension\n",
            "- $M=n_L$ is the output dimension\n",
            "- $r$ is the rank of the \"true\" matrix / teacher $A^*$\n",
            "- $w$ or $H$ is the hidden width (for rectangular networks).\n",
            "- $\\sigma$ is the teacher's output noise. By default, we use $\\sigma=0$.\n",
            "\n",
            "\n",
            "# Set-up\n",
            "\n",
            "- For the definition of the model `DLN`, a `torch.nn.Module`, see `devinterp.zoo.dlns.model`.\n",
            "- For the definition of the dataset `DLNDataset`, a `torch.utils.data.Dataset`, see `devinterp.zoo.dlns.dataset`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "%pip install devinterp matplotlib seaborn"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import logging\n",
            "import os\n",
            "from typing import Callable, Dict, List, Optional\n",
            "\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "import torch\n",
            "from torch import nn\n",
            "from torch import optim\n",
            "from torch.nn import functional as F\n",
            "from tqdm.notebook import tqdm\n",
            "\n",
            "from devinterp.slt import estimate_learning_coeff, estimate_learning_coeff_with_summary\n",
            "from devinterp.zoo.dlns.model import DLN\n",
            "from devinterp.zoo.dlns.data import DLNDataset\n",
            "from devinterp.optim.sgld import SGLD\n",
            "\n",
            "logging.basicConfig(level=logging.INFO)\n",
            "\n",
            "sns.set_palette(\"deep\")\n",
            "sns.set_style(\"whitegrid\")\n",
            "\n",
            "PRIMARY, SECONDARY, TERTIARY = sns.color_palette(\"deep\")[:3]\n",
            "PRIMARY_LIGHT, SECONDARY_LIGHT, TERTIARY_LIGHT = sns.color_palette(\"muted\")[:3]\n",
            "\n",
            "DEVICE = os.environ.get(\n",
            "    \"DEVICE\",\n",
            "    \"cuda:0\"\n",
            "    if torch.cuda.is_available()\n",
            "    else \"mps\"\n",
            "    if torch.backends.mps.is_available()\n",
            "    else \"cpu\",\n",
            ")\n",
            "DEVICE = torch.device(DEVICE)\n",
            "NUM_CORES = int(os.environ.get(\"NUM_CORES\", 1))\n",
            "DEVICE, NUM_CORES"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pydantic import BaseModel\n",
            "from dataclasses import dataclass\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class Learner:\n",
            "    config: \"RectangularDLNConfig\"\n",
            "    model: nn.Module\n",
            "    dataset: torch.utils.data.Dataset\n",
            "    loader: torch.utils.data.DataLoader\n",
            "    optimizer: torch.optim.Optimizer\n",
            "    evals: Callable[[nn.Module], Dict[str, float]]\n",
            "\n",
            "\n",
            "class RectangularDLNConfig(BaseModel):\n",
            "    teacher_matrix: torch.Tensor\n",
            "    gamma: float = 1.1\n",
            "    w: int = 100\n",
            "    L: int = 4\n",
            "    seed: int = 0\n",
            "    noise_level: float = 1.0\n",
            "    num_training_samples: int = 1024\n",
            "    batch_size: int = 128\n",
            "    num_steps: int = 10_000\n",
            "    device: str = \"cpu\"\n",
            "    lr: float = 1e-3\n",
            "    momentum: float = 0.9\n",
            "    weight_decay: float = 1e-3\n",
            "\n",
            "    class Config:\n",
            "        arbitrary_types_allowed = True\n",
            "\n",
            "    def create_teacher(self):\n",
            "        return DLN.from_matrix(self.teacher_matrix, L=1)\n",
            "\n",
            "    def create_student(self):\n",
            "        return DLN.make_rectangular(\n",
            "            input_dim=self.input_dim,\n",
            "            output_dim=self.output_dim,\n",
            "            L=self.L,\n",
            "            w=self.w,\n",
            "            gamma=self.gamma,\n",
            "        )\n",
            "\n",
            "    def create_data(self, teacher: DLN):\n",
            "        return DLNDataset.generate_split(\n",
            "            teacher, self.num_training_samples, self.noise_level, self.seed\n",
            "        )\n",
            "\n",
            "    def create_learner(self, **kwargs):\n",
            "        teacher = self.create_teacher()\n",
            "        student = self.create_student()\n",
            "\n",
            "        for n, p in student.named_parameters():\n",
            "            print(n, p.shape)\n",
            "\n",
            "        trainset, testset = self.create_data(teacher)\n",
            "        trainloader = torch.utils.data.DataLoader(\n",
            "            trainset, batch_size=self.batch_size, shuffle=True\n",
            "        )\n",
            "        evals = make_evals(teacher_matrix, trainset, testset, self.device, **kwargs)\n",
            "        optimizer = optim.SGD(\n",
            "            student.parameters(),\n",
            "            lr=self.lr,\n",
            "            momentum=self.momentum,\n",
            "            weight_decay=self.weight_decay,\n",
            "        )\n",
            "\n",
            "        learner = Learner(self, student, trainset, trainloader, optimizer, evals)\n",
            "        return learner\n",
            "\n",
            "    @property\n",
            "    def input_dim(self):\n",
            "        return self.teacher_matrix.shape[1]\n",
            "\n",
            "    @property\n",
            "    def output_dim(self):\n",
            "        return self.teacher_matrix.shape[0]\n",
            "\n",
            "    def model_dump(self, *args, **kwargs):\n",
            "        dump = super().model_dump(*args, **kwargs)\n",
            "        dump[\"teacher_matrix\"] = self.teacher_matrix.tolist()\n",
            "\n",
            "        return dump\n",
            "\n",
            "\n",
            "def make_evals(\n",
            "    teacher_matrix: torch.Tensor,\n",
            "    trainset: DLNDataset,\n",
            "    testset: DLNDataset,\n",
            "    device: str,\n",
            "    num_draws: int = 10,\n",
            "    num_chains: int = 10,\n",
            "    num_burnin_steps: int = 0,\n",
            "    num_steps_bw_draws: int = 1,\n",
            "    num_cores: int = NUM_CORES,\n",
            "    **kwargs,\n",
            "):\n",
            "    teacher_matrix = teacher_matrix.to(device)\n",
            "\n",
            "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
            "    testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
            "\n",
            "    def eval_mse(model, loader):\n",
            "        loss = 0\n",
            "        count = 0\n",
            "\n",
            "        for x, y in loader:\n",
            "            x, y = x.to(device), y.to(device)\n",
            "            loss += F.mse_loss(model(x), y, reduction=\"sum\").item()\n",
            "            count += len(x)\n",
            "\n",
            "        return loss / count\n",
            "\n",
            "    def eval_progress(model: DLN):\n",
            "        # Divide the first singular value by the first singular value of the teacher, and so on, then sum.\n",
            "        # This needs a new name.\n",
            "        singular_values = model.to_matrix().to(\"cpu\").svd().S\n",
            "        teacher_singular_values = teacher_matrix.to(\"cpu\").svd().S\n",
            "        missing_singular_values = teacher_singular_values == 0\n",
            "        teacher_singular_values[missing_singular_values] = 1\n",
            "        progress = singular_values / teacher_singular_values\n",
            "        # Get rid of division by zero problems\n",
            "        progress[progress == np.inf] = 0\n",
            "        progress[progress == -np.inf] = 0\n",
            "        progress[missing_singular_values] = 0\n",
            "\n",
            "        return torch.sum(progress).item()\n",
            "\n",
            "    def eval_matrix_properties(model: DLN):\n",
            "        return {\n",
            "            \"rank\": model.rank(atol=1e-1).item(),\n",
            "            \"ranks\": [e.item() for e in model.ranks(atol=1e-1)],\n",
            "            \"grad_norm\": model.grad_norm().item(),\n",
            "            \"nuc_norm\": model.norm(p=\"nuc\").item(),\n",
            "            \"nuc_norms\": [e.item() for e in model.norms(p=\"nuc\")],\n",
            "        }\n",
            "\n",
            "    def eval_rlct(model: DLN):\n",
            "        model.to(\"cpu\")\n",
            "        optimizer_kwargs = dict(\n",
            "            lr=1e-4, temperature=\"adaptive\", num_samples=len(trainset), elasticity=1.0\n",
            "        )\n",
            "        optimizer_kwargs.update(kwargs)\n",
            "        rlct = estimate_learning_coeff_with_summary(\n",
            "            model,\n",
            "            loader=trainloader,\n",
            "            criterion=F.mse_loss,\n",
            "            sampling_method=SGLD,\n",
            "            optimizer_kwargs=optimizer_kwargs,\n",
            "            num_draws=num_draws,\n",
            "            num_chains=num_chains,\n",
            "            num_burnin_steps=num_burnin_steps,\n",
            "            num_steps_bw_draws=num_steps_bw_draws,\n",
            "            cores=num_cores,\n",
            "            pbar=False,\n",
            "            device=torch.device(device),\n",
            "        )\n",
            "        model.to(device)\n",
            "        return rlct\n",
            "\n",
            "    def evals(model):\n",
            "        return {\n",
            "            \"mse/train\": eval_mse(model, trainloader),\n",
            "            \"mse/test\": eval_mse(model, testloader),\n",
            "            \"progress\": eval_progress(model),\n",
            "            **eval_matrix_properties(model),\n",
            "            **eval_rlct(model),\n",
            "        }\n",
            "\n",
            "    return evals\n",
            "\n",
            "\n",
            "# teacher_matrix = 10.0 * torch.Tensor(np.diag([1, 2, 3, 4, 5])).detach()\n",
            "teacher_matrix = torch.zeros(5, 5) # + 1e-2 * torch.randn(5, 5)\n",
            "config = RectangularDLNConfig(\n",
            "    teacher_matrix=teacher_matrix,\n",
            "    num_training_samples=1024,\n",
            "    batch_size=128,\n",
            "    num_steps=1_000,\n",
            "    w=5,\n",
            "    L=4,\n",
            "    gamma=1.0,\n",
            "    noise_level=0.0,\n",
            "    device=str(DEVICE),\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "learner = config.create_learner()\n",
            "# df = train(learner)\n",
            "\n",
            "torch.manual_seed(0)    \n",
            "\n",
            "# Set to diagonals \n",
            "for p in learner.model.parameters():\n",
            "    p.data = torch.eye(*p.shape) + 1e-3 * torch.randn(5, 5)\n",
            "    print(p)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm.notebook import tqdm \n",
            "\n",
            "def train(learner):\n",
            "    learner.model.to(learner.config.device)\n",
            "    learner.model.train()\n",
            "\n",
            "    evals = []\n",
            "\n",
            "    num_steps = learner.config.num_steps\n",
            "    logging_steps = set(np.linspace(0, num_steps, 50).astype(int)) | set(\n",
            "        np.logspace(0, num_steps, 50).astype(int)\n",
            "    )\n",
            "\n",
            "    def log(step):\n",
            "        learner.model.eval()\n",
            "        evals.append({\"step\": step, **learner.evals(learner.model)})\n",
            "        # print(yaml.dump(evals[-1]))\n",
            "        learner.model.train()\n",
            "\n",
            "    step = -1\n",
            "    epoch = -1\n",
            "\n",
            "    pbar = tqdm(\n",
            "        total=learner.config.num_steps,\n",
            "        desc=f\"Training...\",\n",
            "    )\n",
            "\n",
            "    while step < learner.config.num_steps:\n",
            "        torch.manual_seed(step)\n",
            "        epoch += 1\n",
            "\n",
            "        for x, y in learner.loader:\n",
            "            step += 1\n",
            "            x, y = x.to(learner.config.device), y.to(learner.config.device)\n",
            "            learner.optimizer.zero_grad()\n",
            "            y_hat = learner.model(x)\n",
            "            loss = F.mse_loss(y_hat, y)\n",
            "            loss.backward()\n",
            "            learner.optimizer.step()\n",
            "\n",
            "            if step in logging_steps:\n",
            "                log(step=step)\n",
            "\n",
            "            pbar.update(1)\n",
            "\n",
            "    if pbar:\n",
            "        pbar.close()\n",
            "\n",
            "    log(step=step)\n",
            "\n",
            "    evals_df = pd.DataFrame(evals)\n",
            "    evals_df.sort_values(\"step\", inplace=True)\n",
            "\n",
            "    return evals_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = train(learner)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_loss_vs_learning_coeff(\n",
            "    df, figsize=(8, 6), title=None, ax: Optional[plt.Axes] = None, xlog=False, std=False\n",
            "):\n",
            "    if not ax:\n",
            "        fig, ax = plt.subplots(figsize=figsize)\n",
            "\n",
            "    ax.set_title(title if title else \"Loss vs. Learning Coefficient\")\n",
            "\n",
            "    # Train error\n",
            "    ax.plot(df.step, df[\"mse/test\"], label=\"Test error\", color=PRIMARY)\n",
            "    ax.plot(df.step, df[\"mse/train\"], label=\"Train error\", color=PRIMARY_LIGHT, alpha=0.5)\n",
            "    ax.set_yscale(\"log\")\n",
            "    ax.set_ylabel(\"MSE\", color=PRIMARY)\n",
            "    ax.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
            "    ax.legend(loc=\"lower right\")\n",
            "\n",
            "    # Learning coefficients\n",
            "    axb = ax.twinx()\n",
            "    rlcts = np.clip(df[\"mean\"].to_numpy(), 0, None)\n",
            "    axb.plot(df.step, rlcts, label=\"RLCTs\", color=SECONDARY)\n",
            "    axb.set_ylabel(r\"Local Learning Coefficient, $\\hat \\lambda$\", color=SECONDARY)\n",
            "    axb.tick_params(axis=\"y\", labelcolor=SECONDARY)\n",
            "\n",
            "    ax.set_xlabel(\"Step\")\n",
            "\n",
            "    if xlog:\n",
            "        ax.set_xscale(\"log\")\n",
            "\n",
            "    if std:\n",
            "        axb.fill_between(\n",
            "            df.step,\n",
            "            df[\"mean\"] - df[\"std\"],\n",
            "            df[\"mean\"] + df[\"std\"],\n",
            "            color=SECONDARY,\n",
            "            alpha=0.3,\n",
            "            label=r\"Std $\\hat\\lambda$\",\n",
            "        )\n",
            "\n",
            "\n",
            "def plot_all(df, xlog=False, figsize=(8, 6), title=None):\n",
            "    L = len(df.ranks[0])\n",
            "\n",
            "    # Figure 1: Loss and RLCTs\n",
            "    fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
            "    ax, ax2 = axes\n",
            "\n",
            "    plot_loss_vs_learning_coeff(df, ax=ax, title=title, xlog=xlog)\n",
            "\n",
            "    # Figure 2: Nuclear Norms\n",
            "    ax2.set_title(title if title else \"Nuclear Norms\")\n",
            "    ax2.set_xlabel(\"Step\")\n",
            "    if xlog:\n",
            "        ax2.set_xscale(\"log\")\n",
            "\n",
            "    # Nuclear Norms\n",
            "    for l in range(L):\n",
            "        ax2.plot(df.step, [e[l] for e in df.nuc_norms], label=f\"Nuclear Norm {l}\")\n",
            "\n",
            "    ax2.set_ylabel(\"Nuclear Norms\")\n",
            "    ax2.legend(loc=\"lower right\")\n",
            "\n",
            "    plt.tight_layout()\n",
            "    plt.show()\n",
            "\n",
            "plot_loss_vs_learning_coeff(df, xlog=False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Let's do some cherrypicking\n",
            "dfs = []\n",
            "\n",
            "for seed in range(5):\n",
            "    config = RectangularDLNConfig(\n",
            "        teacher_matrix=teacher_matrix,\n",
            "        num_training_samples=1024,\n",
            "        batch_size=128,\n",
            "        num_steps=10_000,\n",
            "        w=100,\n",
            "        L=4,\n",
            "        gamma=1.0,\n",
            "        noise_level=0.0,\n",
            "        device=str(DEVICE),\n",
            "        seed=seed,\n",
            "    )\n",
            "    learner = config.create_learner(\n",
            "        num_draws=10, num_chains=100, lr=1e-4, elasticity=1.0, repeats=5\n",
            "    )\n",
            "    df = train(learner)\n",
            "    dfs.append(df)\n",
            "    plot_loss_vs_learning_coeff(df, xlog=False)\n",
            "    plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for df in dfs:\n",
            "    plot_loss_vs_learning_coeff(df, std=True)\n",
            "    plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Experiments"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Defining all the teacher matrices\n",
            "\n",
            "\n",
            "def run_experiment(teacher_matrix: torch.Tensor, seed=None, **kwargs):\n",
            "    if seed:\n",
            "        torch.manual_seed(seed)\n",
            "\n",
            "    config = RectangularDLNConfig(teacher_matrix=teacher_matrix, **kwargs)\n",
            "    learner = config.create_learner()\n",
            "    df = train(learner)\n",
            "    return df\n",
            "\n",
            "\n",
            "# Set up the teacher matrices\n",
            "\n",
            "rk5_matrix = torch.Tensor(10 * np.diag(np.arange(1, 6)))\n",
            "\n",
            "rk4_matrix = rk5_matrix.clone()\n",
            "rk4_matrix[-1, -1] = 0\n",
            "\n",
            "rk2_matrix = rk4_matrix.clone()\n",
            "rk2_matrix[-2, -2] = 0\n",
            "rk2_matrix[-3, -3] = 0\n",
            "\n",
            "default_settings = dict(\n",
            "    num_training_samples=1024,\n",
            "    batch_size=128,\n",
            "    num_steps=10_000,\n",
            "    w=100,\n",
            "    L=4,\n",
            "    gamma=1.0,\n",
            "    noise_level=0.0,\n",
            "    device=str(DEVICE),\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "results = {}\n",
            "SEED = 0\n",
            "\n",
            "for rk, teacher_matrix in zip([5, 4, 2], [rk5_matrix, rk4_matrix, rk2_matrix]):\n",
            "    for noise_level in [0.0, 10.0]:\n",
            "        name = f\"rk{rk}_L4_w100_noise{noise_level}\"\n",
            "        results[name] = run_experiment(rk5_matrix, seed=SEED, **default_settings)\n",
            "        plot_all(results[name], xlog=False, title=f\"r={rk}, L=4, w=100, noise={noise_level}\")\n",
            "\n",
            "df = None\n",
            "\n",
            "for rk, teacher_matrix in zip([5, 4, 2], [rk5_matrix, rk4_matrix, rk2_matrix]):\n",
            "    for noise_level in [0.0, 10.0]:\n",
            "        _df = pd.DataFrame(results[f\"rk{rk}_L4_w100_noise{noise_level}\"])\n",
            "        _df[\"r\"] = rk\n",
            "        _df[\"noise_level\"] = noise_level\n",
            "\n",
            "        df = pd.concat([df, _df]) if df is not None else _df\n",
            "\n",
            "df"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Recreate figure 5"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "\n",
            "def plot_grid(\n",
            "    df,\n",
            "    x_axis: str,\n",
            "    z_axis: str,\n",
            "    metrics: List[str],\n",
            "    title: str,\n",
            "    logscale: bool = True,\n",
            "    inset=False,\n",
            "    figsize=(10, 6),\n",
            "):\n",
            "    xs = df[x_axis].unique()\n",
            "    zs = df[z_axis].unique()\n",
            "\n",
            "    # Define the colors for each w value\n",
            "    colors = [PRIMARY, SECONDARY, TERTIARY]\n",
            "\n",
            "    # Create a figure with 3 subplots (one for each gamma)\n",
            "    fig, axes = plt.subplots(len(metrics), 3, figsize=figsize)\n",
            "    fig.suptitle(title)\n",
            "\n",
            "    fig.tight_layout()\n",
            "\n",
            "    # Iterate through the unique gammas\n",
            "    for i, x in enumerate(xs):\n",
            "        for j, metric in enumerate(metrics):\n",
            "            axes[j, 0].set_ylabel(metric)\n",
            "            axes[-1, i].set_xlabel(\"# of steps\")\n",
            "\n",
            "            ax = axes[j, i]\n",
            "            # Add an inset focusing on the first 2000 steps\n",
            "            ax_inset = ax.inset_axes([0.65, 0.7, 0.3, 0.25])\n",
            "\n",
            "            for k, z in enumerate(zs):\n",
            "                data = df[(df[x_axis] == x) & (df[z_axis] == z)]\n",
            "                color = colors[k]\n",
            "\n",
            "                # Plot the training error against the number of steps\n",
            "                ax.plot(data.step, data[metric], color=color, label=f\"{z_axis}={z}\")\n",
            "\n",
            "                inset_data = data.loc[data.step < 2000]\n",
            "                ax_inset.plot(inset_data.step, inset_data[metric], color=color)\n",
            "\n",
            "            ax_inset.yaxis.set_visible(False)\n",
            "            ax_inset.xaxis.set_visible(False)\n",
            "\n",
            "            if logscale:\n",
            "                ax_inset.set_yscale(\"log\")\n",
            "                ax.set_yscale(\"log\")\n",
            "                # ax_inset.set_xscale('log')\n",
            "                # ax.set_xscale('log')\n",
            "\n",
            "            if not inset:\n",
            "                ax_inset.remove()\n",
            "\n",
            "            ax.set_title(f\"{x_axis}={x}\")\n",
            "            ax.legend(loc=\"lower left\")\n",
            "\n",
            "    plt.show()\n",
            "\n",
            "\n",
            "plot_grid(\n",
            "    df,\n",
            "    \"r\",\n",
            "    \"noise_level\",\n",
            "    [\"mse/train\", \"rlct/mean\", \"nuc_norm\"],\n",
            "    \"Rank in [5, 4, 2], Noise Level in [0., 10.]\",\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "fig5_df = None\n",
            "\n",
            "fig5_settings = dict(\n",
            "    num_training_samples=1024,\n",
            "    batch_size=128,\n",
            "    num_steps=10_000,\n",
            "    L=4,\n",
            "    noise_level=0.0,\n",
            "    device=str(DEVICE),\n",
            ")\n",
            "\n",
            "for gamma in [0.75, 1.0, 1.5]:\n",
            "    # for w in [10, 100, 1000]:\n",
            "    for w in [10, 100]:\n",
            "        results = run_experiment(rk5_matrix, seed=SEED, w=w, gamma=gamma, **fig5_settings)\n",
            "        _df = pd.DataFrame(results)\n",
            "        _df[\"w\"] = w\n",
            "        _df[\"gamma\"] = gamma\n",
            "        fig5_df = pd.concat([fig5_df, _df]) if fig5_df is not None else _df\n",
            "        plot_all(results, xlog=False, title=f\"r=5, L=4, w={w}, noise=0, gamma={gamma}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_grid(\n",
            "    fig5_df,\n",
            "    \"gamma\",\n",
            "    \"w\",\n",
            "    [\"mse/train\", \"rlct/mean\", \"nuc_norm\"],\n",
            "    \"Gamma in [0.75, 1.0, 1.5], w in [10, 100, 1000]\",\n",
            ")\n",
            "fig5_df"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.15"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
